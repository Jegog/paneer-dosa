{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "013238e3-435f-4f7c-ac00-69bc9ee284b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#テキストファイルを読み込んで、形態素解析\n",
    "#ファイルはcsv(UTF-8)、識別フラグ・テキスト・除外単語の3項目\n",
    "#除外項目は必要に応じ個別指定\n",
    "#識別フラグ×単語数、識別フラグ×共起パターンをファイル出力\n",
    "#指定した識別タグについて、ワードクラウド、共起ネットワークを出力・表示\n",
    "#各識別タグごとに、ワードクラウド、共起ネットワークを出力・表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd776aa-8f5d-47bd-84cc-a697c5d1562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ライブラリのインポート\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import MeCab\n",
    "import fitz\n",
    "import sys\n",
    "sys.path.append('/work_dir') \n",
    "import talknize_module_20240909 as tk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "269801eb-e30c-49ef-abca-20db4538d85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flag</th>\n",
       "      <th>text</th>\n",
       "      <th>exclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text_a</td>\n",
       "      <td>貴社が汽車で帰社した。蛙の子は蛙。</td>\n",
       "      <td>adm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Text_b</td>\n",
       "      <td>隣の客はよく柿食う客だ。柿といえば秋の果物\\n柿食えば金がなるなり法隆寺。</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     flag                                   text exclusion\n",
       "0  Text_a                      貴社が汽車で帰社した。蛙の子は蛙。       adm\n",
       "1  Text_b  隣の客はよく柿食う客だ。柿といえば秋の果物\\n柿食えば金がなるなり法隆寺。       NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#分析対象ファイル読込み\n",
    "\n",
    "directory_name=\"data/\"\n",
    "file_name= 'sample.csv'\n",
    "target = directory_name + file_name\n",
    "\n",
    "#CSVファイルのヘッダ有無を指定 ヘッダあり：1,ヘッダなし0\n",
    "header_flag=1\n",
    "\n",
    "#ヘッダ有無をふまえてファイル読込み\n",
    "if header_flag==1:\n",
    "    #ヘッダ情報あり\n",
    "    df_targets = pd.read_csv(target, dtype=str)\n",
    "else:\n",
    "    #ヘッダ情報なし\n",
    "    df_targets = pd.read_csv(target, header=None, dtype=str)\n",
    "    df_targets.columns = ['flag', 'text', 'exclusion']# データフレームのカラム名を後付けで定義\n",
    "\n",
    "df_targets.head()\n",
    "#print(df_targets.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "764db6ba-b14d-4282-af65-876c00744274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#形態素解析の前処理\n",
    "#分析対象データの特性に対応するため、前処理はモジュールではなくコードとして記載\n",
    "#talknize_module.pyにも標準的な処理も記載し、適宜使い分け可能にする\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "#テキストファイルの各行に記載された文字列を、処理用文字列として整形・リスト化\n",
    "def file_to_list(file_path):\n",
    "\n",
    "    # 空のリストを作成\n",
    "    return_list = []\n",
    "    try:\n",
    "        # 指定されたファイルを読み込み、各行をリストに追加\n",
    "        with open(file_path, 'r') as file:\n",
    "            # ファイル内の各行をループし、行末の改行や余分な空白を除去してリストに格納\n",
    "            return_list = [line.strip() for line in file]\n",
    "    # ファイルが存在しない場合は例外を無視する\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    # リストを返す\n",
    "    return return_list\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "#形態素解析前のテキストデータ処理（\n",
    "#形態素解析の前に、無駄な記号やヘッダ・フッタ等の文言をテキストから除外\n",
    "def pre_tk(text):\n",
    "\n",
    "    replaced_text = text\n",
    "\n",
    "    #exclusion_list処理前に処理する必要のあるもの\n",
    "    #【特例処理】除外処理前に、文頭のこれら記号は「箇条書き」とみなし、続く文言を一文として扱う\n",
    "    replaced_text = re.sub(r'[■□▪▫▲△▶▷▸▹▼▽◆◇●〇]', '。\\n', replaced_text)\n",
    "    #replaced_text = re.sub(r'[〇●◇◆□■▶△▲▽▼▫▪▹▶▸]', '', replaced_text)#上記以外は除去\n",
    "\n",
    "    exclusion_list = []    \n",
    "    exclusion_file1 = \"userdic/exclusion_phrases1.txt\"  # 各企業の除外フレーズを記載したリスト\n",
    "    #exclusion_file2 = \"userdic/exclusion_phrases2.txt\"  # 各企業の除外フレーズを記載したリスト2（pageinfoから都度取り込み）\n",
    "    #exclusion_file3 = \"userdic/exclusion_codes.txt\"  # その他記号・年月日・URL等を除外するためのリスト\n",
    "    exclusion_list = file_to_list(exclusion_file1)# + fileto_list(exclusion_file2)+ file_to_list(exclusion_file3)\n",
    "\n",
    "    for pattern in exclusion_list:\n",
    "        replaced_text = re.sub(pattern, ' ', replaced_text)\n",
    "\n",
    "    return replaced_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e29d06-b6f4-4ad6-92eb-496a8e83ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#形態素解析の後処理\n",
    "#形態素解析結果（tokenリスト）から、ストップワード、特定の条件の文字列等を除外\n",
    "#import fitz\n",
    "#import re\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "\n",
    "def post_tk(tokens):\n",
    "    \n",
    "    replaced_list = tokens    \n",
    "\n",
    "    # stopwords（ファイルに格納）を除去\n",
    "    path_stopwords = \"userdic/stopwords.txt\"\n",
    "    stopwords = file_to_list(path_stopwords)\n",
    "    stopwords = list(OrderedDict.fromkeys(stopwords)) # 元の順序を保持しつつ、重複を除去（# Python 3.7以降）\n",
    "    replaced_list = [t for t in replaced_list if t not in stopwords]\n",
    "    \n",
    "    # ひらがなのみの要素を除去\n",
    "    kana_re = re.compile(\"^[ぁ-ゖ]+$\")\n",
    "    replaced_list = [t for t in replaced_list if not kana_re.match(t)]\n",
    "\n",
    "    # アルファベット1文字のみの要素を除去\n",
    "    alphabet_re = re.compile(\"^[a-zA-Z]$\")\n",
    "    replaced_list = [t for t in replaced_list if not alphabet_re.match(t)]\n",
    "\n",
    "    #特定の形態の数値要素を除去\n",
    "    number_re = re.compile(\"^[\\d,]+\")\n",
    "    replaced_list = [t for t in replaced_list if not number_re.match(t)]\n",
    "\n",
    "    \n",
    "    return replaced_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68497c62-d4ab-4fc3-b252-42af47c2bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#リストを受けて除外する処理を関数化\n",
    "def post_tk_sp(token, exclusion_str):\n",
    "    replaced_list = token\n",
    "\n",
    "    # exclusion_strが文字列かどうかを確認し、そうでない場合は空リストにする\n",
    "    if isinstance(exclusion_str, str):\n",
    "        exclusion_list = exclusion_str.split(\"\\n\")\n",
    "    else:\n",
    "        exclusion_list = []\n",
    "\n",
    "    # exclusion_listにないものだけを出力\n",
    "    replaced_list = [item for item in replaced_list if item not in exclusion_list]\n",
    "\n",
    "    return replaced_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f96194b8-b619-4386-aa50-e3f7eea5003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#抽出単語によるワードクラウド作成\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#単語と件数のデータフレームをインプットとして、ワードクラウドを出力\n",
    "def WCFreq(flag_string, df_wordcount, plotflag):\n",
    "    # 日本語フォントのパスを指定\n",
    "    jp_font_path = '/usr/share/fonts/opentype/ipaexfont-gothic/ipaexg.ttf'\n",
    "    \n",
    "    # ワードクラウドのフォーマット指定\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white',font_path=jp_font_path)\n",
    "    # 単語とその頻度を辞書形式に変換\n",
    "    word_freq = {word: freq for word, freq in zip(df_wordcount['単語'], df_wordcount['件数'])}\n",
    "    # ワードクラウドの生成\n",
    "    wordcloud.generate_from_frequencies(word_freq)\n",
    "    \n",
    "    # プロット\n",
    "    if plotflag==True:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    # 結果をpngファイルに出力\n",
    "    file_name_wordcloud=f'output/{flag_string}_WordCloud.png'\n",
    "    wordcloud.to_file(file_name_wordcloud)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a35cd3c-0913-4370-84e4-ee9c7d8a5078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     flag                            token\n",
      "0  Text_a              [貴社, 汽車, 帰社, 蛙の子は蛙]\n",
      "1  Text_b  [隣, 客, 柿, 客, 柿, 秋の, 果物, 柿, 法隆寺]\n"
     ]
    }
   ],
   "source": [
    "#flag単位で、形態素解析と集計結果を出力\n",
    "\n",
    "# flagとtokenのみのDataFrameも用意しておく\n",
    "df_tokens = pd.DataFrame(columns=['flag', 'token'])\n",
    "\n",
    "for idx, row in df_targets.iterrows():\n",
    "    flag_value = row.flag\n",
    "    text_value = row.text\n",
    "    exclusion_value = row.exclusion\n",
    "\n",
    "    # token_listの作成\n",
    "    token_list = [flag_value, post_tk_sp(post_tk(tk.mecab_tokenizer(pre_tk(text_value))), exclusion_value)]\n",
    "\n",
    "    # 新しい行をpd.Seriesとして追加\n",
    "    new_row = pd.Series(token_list, index=df_tokens.columns)\n",
    "\n",
    "    # pd.concatを使ってDataFrameに新しい行を追加\n",
    "    df_tokens = pd.concat([df_tokens, new_row.to_frame().T], ignore_index=True)\n",
    "\n",
    "print(df_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ad7edf5-ba28-4ca9-8f7e-e53a1e2d28d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flag</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text_a</td>\n",
       "      <td>[貴社, 汽車, 帰社, 蛙の子は蛙]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Text_b</td>\n",
       "      <td>[隣, 客, 柿, 客, 柿, 秋の, 果物, 柿, 法隆寺]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     flag                            token\n",
       "0  Text_a              [貴社, 汽車, 帰社, 蛙の子は蛙]\n",
       "1  Text_b  [隣, 客, 柿, 客, 柿, 秋の, 果物, 柿, 法隆寺]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flag単位で、形態素解析と集計結果を出力\n",
    "#csvを読み込んだDataFrameに、形態素解析結果を列で追加\n",
    "\n",
    "#読み込んだデータフレームに'token'を追加\n",
    "df_targets['token']=''\n",
    "\n",
    "for idx, row in df_targets.iterrows():\n",
    "    flag_value = row.flag\n",
    "    text_value = row.text\n",
    "    exclusion_value = row.exclusion\n",
    "\n",
    "    # token_listの作成\n",
    "    token_list = post_tk_sp(post_tk(tk.mecab_tokenizer(pre_tk(text_value))), exclusion_value)\n",
    "\n",
    "    # 各行に対応するトークンを代入\n",
    "    df_targets.at[idx, 'token'] = token_list\n",
    "\n",
    "df_targets\n",
    "df_targets[['flag','token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92ecc5a4-6faf-48cf-bdf3-de1f4082f83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flag</th>\n",
       "      <th>単語</th>\n",
       "      <th>件数</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text_a</td>\n",
       "      <td>貴社</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Text_a</td>\n",
       "      <td>汽車</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Text_a</td>\n",
       "      <td>帰社</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Text_a</td>\n",
       "      <td>蛙の子は蛙</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Text_b</td>\n",
       "      <td>柿</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Text_b</td>\n",
       "      <td>客</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Text_b</td>\n",
       "      <td>隣</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Text_b</td>\n",
       "      <td>秋の</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Text_b</td>\n",
       "      <td>果物</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Text_b</td>\n",
       "      <td>法隆寺</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     flag     単語 件数\n",
       "0  Text_a     貴社  1\n",
       "1  Text_a     汽車  1\n",
       "2  Text_a     帰社  1\n",
       "3  Text_a  蛙の子は蛙  1\n",
       "4  Text_b      柿  3\n",
       "5  Text_b      客  2\n",
       "6  Text_b      隣  1\n",
       "7  Text_b     秋の  1\n",
       "8  Text_b     果物  1\n",
       "9  Text_b    法隆寺  1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataFrameの行ごとに、形態素の件数を集計\n",
    "from collections import Counter\n",
    "\n",
    "word_count = pd.DataFrame(columns=['flag','単語','件数'])\n",
    "\n",
    "for idx, row in df_targets.iterrows():\n",
    "    flag_value = row.flag\n",
    "    text_value = row.text\n",
    "    exclusion_value = row.exclusion\n",
    "    token_value = row.token\n",
    "\n",
    "    # 辞書形式で単語をカウント\n",
    "    counter = Counter(token_value)\n",
    "    # 単語、件数をDataFrameに格納\n",
    "    count_df = pd.DataFrame(list(counter.items()), columns=['単語', '件数'])\n",
    "    # DataFrameを件数でソート\n",
    "    count_df = count_df.sort_values(by='件数', ascending=False)\n",
    "\n",
    "    # flagを加えて集計結果のデータフレームに格納\n",
    "    count_df['flag'] = flag_value\n",
    "    columns_order = ['flag', '単語', '件数']\n",
    "    count_df = count_df[columns_order]\n",
    "    word_count = pd.concat([word_count, count_df], ignore_index=True)\n",
    "\n",
    "    # flag単位でファイル出力\n",
    "    file_name_word_count = f\"output/{flag_value}_Word_Count.csv\"\n",
    "    count_df.to_csv(file_name_word_count, encoding=\"utf_8_sig\", index=False)    \n",
    "    \n",
    "    #定義したモジュールでWordCloudも出力\n",
    "    WCFreq(flag_value, count_df, False)\n",
    "\n",
    "word_count  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd7be0ad-dac2-447d-a5f9-42a0103a12d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flag</th>\n",
       "      <th>単語1</th>\n",
       "      <th>単語2</th>\n",
       "      <th>件数</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text_a</td>\n",
       "      <td>汽車</td>\n",
       "      <td>貴社</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Text_a</td>\n",
       "      <td>帰社</td>\n",
       "      <td>貴社</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Text_a</td>\n",
       "      <td>帰社</td>\n",
       "      <td>汽車</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Text_b</td>\n",
       "      <td>客</td>\n",
       "      <td>隣</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Text_b</td>\n",
       "      <td>客</td>\n",
       "      <td>柿</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Text_b</td>\n",
       "      <td>柿</td>\n",
       "      <td>隣</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Text_b</td>\n",
       "      <td>客</td>\n",
       "      <td>客</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Text_b</td>\n",
       "      <td>柿</td>\n",
       "      <td>秋の</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Text_b</td>\n",
       "      <td>果物</td>\n",
       "      <td>柿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Text_b</td>\n",
       "      <td>果物</td>\n",
       "      <td>秋の</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Text_b</td>\n",
       "      <td>柿</td>\n",
       "      <td>法隆寺</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      flag 単語1  単語2 件数\n",
       "0   Text_a  汽車   貴社  1\n",
       "1   Text_a  帰社   貴社  1\n",
       "2   Text_a  帰社   汽車  1\n",
       "3   Text_b   客    隣  2\n",
       "4   Text_b   客    柿  2\n",
       "5   Text_b   柿    隣  1\n",
       "6   Text_b   客    客  1\n",
       "7   Text_b   柿   秋の  1\n",
       "8   Text_b  果物    柿  1\n",
       "9   Text_b  果物   秋の  1\n",
       "10  Text_b   柿  法隆寺  1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#共起分析\n",
    "#テキストを「。」or「\\n」で区切り、文章単位のリストとして格納\n",
    "import itertools\n",
    "\n",
    "pair_count =  pd.DataFrame(columns=['flag','単語1','単語2','件数'])\n",
    "\n",
    "for idx, row in df_targets.iterrows():\n",
    "    flag_value = row.flag\n",
    "    text_value = row.text\n",
    "    exclusion_value = row.exclusion\n",
    "    token_value = row.token\n",
    "\n",
    "    # テキストデータを「。」or「\\n」で区切ったものを sentence とし、それを形態素解析したものを格納\n",
    "    sentences = re.split(r'[。\\n]', pre_tk(text_value))  # 文を分割\n",
    "    tokenized_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = tk.mecab_tokenizer(sentence)  # 形態素解析\n",
    "        if isinstance(tokens, list):  # リストで返ってくることを確認\n",
    "            cleaned_tokens = post_tk(tokens)  # post_tkでトークンをフィルタリング\n",
    "            final_tokens = post_tk_sp(cleaned_tokens, exclusion_value)  # exclusion_valueでフィルタリング\n",
    "            tokenized_sentences.append(final_tokens)\n",
    "\n",
    "    #print(tokenized_sentences)\n",
    "\n",
    "    #各文中の、形態素組み合わせを作る\n",
    "    token_pairs = [list(itertools.combinations(token,2)) for token in tokenized_sentences]\n",
    "    #print(token_combs)\n",
    "    \n",
    "    #組み合わせた2つの形態素の並びをソート\n",
    "    sorted_pairs = [[tuple(sorted(token)) for token in pair] for pair in token_pairs]\n",
    "    #for pair in token_combs: token_combs内の各文に対して処理\n",
    "    #for pair in token_comb: 各文内の各組み合わせ（例えば、(a, b)）に対して処理を行います。\n",
    "    #sorted(token): 各組み合わせ（タプル）内のトークンをソート\n",
    "    #tuple(sorted(token)): ソートされたトークンをタプルに変換（リストがタプルに変換されることで、タプル内の順序を保証）\n",
    "    #print(sorted_combs)\n",
    "\n",
    "    target_pairs = []\n",
    "    for pair in sorted_pairs:\n",
    "        target_pairs.extend(pair)\n",
    "    ct = Counter(target_pairs)\n",
    "    #print(ct)\n",
    "    \n",
    "    # 上位から取得する件数を制限する場合は、数値を指定\n",
    "    most_common_items = ct.most_common()\n",
    "    pair_df = pd.DataFrame([{\"単語1\" : i[0][0], \"単語2\": i[0][1], \"件数\":i[1]} for i in most_common_items])\n",
    "\n",
    "    #共起カウントのデータフレームにflagとともに格納\n",
    "    \n",
    "    # flagを加えて集計結果のデータフレームに格納\n",
    "    pair_df['flag'] = flag_value\n",
    "    columns_order = ['flag', '単語1', '単語2', '件数']\n",
    "    pair_df = pair_df[columns_order]\n",
    "    pair_count = pd.concat([pair_count, pair_df], ignore_index=True)\n",
    "    \n",
    "    #flag単位でファイル出力\n",
    "    file_name_pair = f\"output/{flag_value}_Co_Occurrence.csv\"\n",
    "    pair_df.to_csv(file_name_pair, encoding=\"utf_8_sig\", index=False)\n",
    "\n",
    "pair_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d667649-46c9-4687-a503-b180be571eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ネットワーク分析の下準備\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import girvan_newman\n",
    "import network_plot_module as npm\n",
    "import json\n",
    "\n",
    "#########################################################\n",
    "# 分析対象とする共起単語の組み合わせ（ノード）の上位件数を指定\n",
    "analyzed_links=350\n",
    "\n",
    "# 各flagごとにノードの上位件数までを抽出\n",
    "pair_count['順位'] = pair_count.groupby('flag')['件数'].rank(method='dense', ascending=False)\n",
    "\n",
    "limited_df = pair_count[pair_count['順位'] <= analyzed_links]\n",
    "#########################################################\n",
    "\n",
    "\n",
    "# flag単位でネットワーク分析\n",
    "\n",
    "unified_centralities =  pd.DataFrame(columns=[\n",
    "        'flag',\n",
    "        'Degree Centrality',\n",
    "        'Betweenness Centrality',\n",
    "        'Closeness Centrality',\n",
    "        'Eigenvector Centrality',\n",
    "        'Katz Centrality',\n",
    "        'Community'])\n",
    "\n",
    "\n",
    "for idx, row in df_targets.iterrows():\n",
    "    flag_value = row.flag\n",
    "    text_value = row.text\n",
    "    exclusion_value = row.exclusion\n",
    "    token_value = row.token\n",
    "\n",
    "    network_df = limited_df[limited_df['flag'] == flag_value]\n",
    "    # DataFrameからネットワークのグラフオブジェクトを作成\n",
    "    G = nx.from_pandas_edgelist(network_df, '単語1', '単語2', ['件数'])\n",
    "    # ノードを確認\n",
    "    #print(G.nodes)\n",
    "    # エッジを確認\n",
    "    #print(G.edges(data=True))  # エッジとその属性（件数）も表示\n",
    "\n",
    "    # 各ノードの中心性を計算\n",
    "    try:\n",
    "        degree_centrality = nx.degree_centrality(G)\n",
    "    except:\n",
    "        degree_centrality = {node: '' for node in G.nodes()}\n",
    "        \n",
    "    try:\n",
    "        betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    except:\n",
    "        betweenness_centrality = {node: '' for node in G.nodes()}\n",
    "    \n",
    "    try:\n",
    "        closeness_centrality = nx.closeness_centrality(G)\n",
    "    except:\n",
    "        closeness_centrality = {node: '' for node in G.nodes()}\n",
    "    \n",
    "    try:\n",
    "        eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "    except:\n",
    "        eigenvector_centrality = {node: '' for node in G.nodes()}\n",
    "    \n",
    "    try:\n",
    "        katz_centrality = nx.katz_centrality(G)\n",
    "    except:\n",
    "        katz_centrality = {node: '' for node in G.nodes()}\n",
    "    \n",
    "    # Girvan-Newmanアルゴリズムでコミュニティに分割\n",
    "    comp = girvan_newman(G)\n",
    "    communities = tuple(sorted(c) for c in next(comp))\n",
    "    \n",
    "    # 各ノードがどのコミュニティに属するかを記録\n",
    "    community_map = {}\n",
    "    for i, community in enumerate(communities):\n",
    "        for node in community:\n",
    "            community_map[node] = i\n",
    "    \n",
    "    # 中心性を新しいデータフレームに格納\n",
    "    centrality_df = pd.DataFrame({\n",
    "        'flag': flag_value,\n",
    "        'Node': list(G.nodes()),\n",
    "        'Degree Centrality': [degree_centrality[node] for node in G.nodes()],\n",
    "        'Betweenness Centrality': [betweenness_centrality[node] for node in G.nodes()],\n",
    "        'Closeness Centrality': [closeness_centrality[node] for node in G.nodes()],\n",
    "        'Eigenvector Centrality': [eigenvector_centrality[node] for node in G.nodes()],\n",
    "        'Katz Centrality': [katz_centrality[node] for node in G.nodes()],\n",
    "        'Community': [community_map[node] for node in G.nodes()]  # コミュニティ情報を追加\n",
    "        })\n",
    "    #print(centrality_df)\n",
    "\n",
    "    unified_centralities = pd.concat([unified_centralities, centrality_df], ignore_index=True)\n",
    "    \n",
    "    file_name_comb = f\"output/{flag_value}_Centrality_{analyzed_links}.csv\"\n",
    "    centrality_df.to_csv(file_name_comb, encoding=\"utf_8_sig\", index=False)    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cef1b17e-cfdd-4b6a-a0bf-c7c862d9dc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ネットワーク分析の下準備\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import girvan_newman\n",
    "import network_plot_module as npm\n",
    "import json\n",
    "\n",
    "#########################################################\n",
    "# 分析対象とする共起単語の組み合わせ（ノード）の上位件数を指定\n",
    "analyzed_links=350\n",
    "\n",
    "# 各flagごとにノードの上位件数までを抽出\n",
    "pair_count['順位'] = pair_count.groupby('flag')['件数'].rank(method='dense', ascending=False)\n",
    "\n",
    "limited_df = pair_count[pair_count['順位'] <= analyzed_links]\n",
    "#########################################################\n",
    "\n",
    "\n",
    "# flag単位でネットワーク分析\n",
    "\n",
    "unified_centralities =  pd.DataFrame(columns=[\n",
    "        'flag',\n",
    "        'Degree Centrality',\n",
    "        'Betweenness Centrality',\n",
    "        'Closeness Centrality',\n",
    "        'Eigenvector Centrality',\n",
    "        'Katz Centrality',\n",
    "        'Community'])\n",
    "\n",
    "\n",
    "for idx, row in df_targets.iterrows():\n",
    "    flag_value = row.flag\n",
    "    text_value = row.text\n",
    "    exclusion_value = row.exclusion\n",
    "    token_value = row.token\n",
    "\n",
    "    network_df = limited_df[limited_df['flag'] == flag_value]\n",
    "\n",
    "    # DataFrameからネットワークのグラフオブジェクトを作成\n",
    "    G = nx.from_pandas_edgelist(network_df, '単語1', '単語2', ['件数'])\n",
    "    #print(G.nodes) # ノードを確認\n",
    "    #print(G.edges(data=True))  # エッジとその属性（件数）も表示\n",
    "\n",
    "    # 各ノードの中心性を計算\n",
    "    try:\n",
    "        degree_centrality = nx.degree_centrality(G)\n",
    "    except:\n",
    "        degree_centrality = {node: '' for node in G.nodes()}\n",
    "        \n",
    "    try:\n",
    "        betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    except:\n",
    "        betweenness_centrality = {node: '' for node in G.nodes()}\n",
    "    \n",
    "    try:\n",
    "        closeness_centrality = nx.closeness_centrality(G)\n",
    "    except:\n",
    "        closeness_centrality = {node: '' for node in G.nodes()}\n",
    "    \n",
    "    try:\n",
    "        eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "    except:\n",
    "        eigenvector_centrality = {node: '' for node in G.nodes()}\n",
    "    \n",
    "    try:\n",
    "        katz_centrality = nx.katz_centrality(G)\n",
    "    except:\n",
    "        katz_centrality = {node: '' for node in G.nodes()}\n",
    "    \n",
    "    # Girvan-Newmanアルゴリズムでコミュニティに分割\n",
    "    comp = girvan_newman(G)\n",
    "    communities = tuple(sorted(c) for c in next(comp))\n",
    "    \n",
    "    # 各ノードがどのコミュニティに属するかを記録\n",
    "    community_map = {}\n",
    "    for i, community in enumerate(communities):\n",
    "        for node in community:\n",
    "            community_map[node] = i\n",
    "    \n",
    "    # 中心性を新しいデータフレームに格納\n",
    "    centrality_df = pd.DataFrame({\n",
    "        'flag': flag_value,\n",
    "        'Node': list(G.nodes()),\n",
    "        'Degree Centrality': [degree_centrality[node] for node in G.nodes()],\n",
    "        'Betweenness Centrality': [betweenness_centrality[node] for node in G.nodes()],\n",
    "        'Closeness Centrality': [closeness_centrality[node] for node in G.nodes()],\n",
    "        'Eigenvector Centrality': [eigenvector_centrality[node] for node in G.nodes()],\n",
    "        'Katz Centrality': [katz_centrality[node] for node in G.nodes()],\n",
    "        'Community': [community_map[node] for node in G.nodes()]  # コミュニティ情報を追加\n",
    "        })\n",
    "    #print(centrality_df)\n",
    "\n",
    "    unified_centralities = pd.concat([unified_centralities, centrality_df], ignore_index=True)\n",
    "    \n",
    "    file_name_comb = f\"output/{flag_value}_Centrality_{analyzed_links}.csv\"\n",
    "    centrality_df.to_csv(file_name_comb, encoding=\"utf_8_sig\", index=False)    \n",
    "\n",
    "\n",
    "    #graph用のDataframeサブセットを作成\n",
    "    graph_df = network_df[['単語1', '単語2', '件数']]\n",
    "    #print(graph_df)\n",
    "    \n",
    "    #ネットワーク図を描画、ファイル出力\n",
    "    got_net = npm.kyoki_word_network(graph_df)\n",
    "    #フィルタボタンを表示させる場合は、set_optionを無効にする必要あり\n",
    "    #got_net.show_buttons(filter_=['physics'])\n",
    "    got_net.set_options(\"\"\"\n",
    "    const options = {\n",
    "      \"physics\": {\n",
    "        \"forceAtlas2Based\": {\n",
    "          \"centralGravity\": 0.1,\n",
    "          \"springLength\": 25,\n",
    "          \"springConstant\": 0.1\n",
    "        },\n",
    "        \"minVelocity\": 0.75,\n",
    "        \"solver\": \"forceAtlas2Based\"\n",
    "      }\n",
    "    }\n",
    "    \"\"\")\n",
    "    file_name_kyoki = f'output/{flag_value}_kyoki_{analyzed_links}.html'\n",
    "    got_net.show(file_name_kyoki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58610a-e30b-4020-a652-0f6556722d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
