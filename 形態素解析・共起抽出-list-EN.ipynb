{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a198b08-f8c0-4b10-84ef-7e9f2f4ec1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be205264-7d62-4cce-8932-1f3fefcfb4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tag import UnigramTagger #別途要否確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7037a255-ee56-43cd-9d72-ff497d5ac805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop wordsの読込\n",
    "#後日ここにユーザー独自のstop wordsを加えるようにする\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#tokenから、定義したstop_wordsとハイパーリンク等の除去、pos_tagの付与後、トークンを正規化\n",
    "#　返すのは(token, pos_tag)\n",
    "# lemmatizerの引数として使うpos_tagは読み込んだ時点のもの\n",
    "#（lemmatize後のtokenでpos_tagを取得すると、結果は異なる可能性）\n",
    "\n",
    "def remove_noise(tokens, stop_words=()):\n",
    "    '''Remove @ mentions, hyperlinks, punctuation, and stop words'''\n",
    "\n",
    "    clean_tokens = []\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        # Remove Hyperlinks\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|' \\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)\n",
    "        # Remove handles\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\", \"\", token)\n",
    "\n",
    "        # Lemmatizerの仕様に合わせ、tokenのposタグを変換\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        # Normalize sentence\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            # Get lowercase\n",
    "            clean_tokens.append((token, tag))  # (token, pos_tag) のタプルをリストに追加\n",
    "\n",
    "    return clean_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71b8b6e0-06c3-49bb-8515-e7ae2b5a65fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), ('for', 'IN'), ('text', 'NN'), ('mining', 'NN'), ('in', 'IN'), ('Python', 'NNP'), ('provided', 'VBN'), ('by', 'IN'), ('Panasonic', 'NNP'), ('Holdings', 'NNPS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sample sentence for text mining in Python provided by Panasonic Holdings .\"\n",
    "\n",
    "# テキストをトークン化\n",
    "tokens = word_tokenize(text)\n",
    "# 品詞タグ付け\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70315523-4805-438b-8353-1eeab59a7ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sample', 'JJ'), ('sentence', 'NN'), ('text', 'NN'), ('mining', 'NN'), ('Python', 'NNP'), ('provide', 'VBN'), ('Panasonic', 'NNP'), ('Holdings', 'NNPS')]\n"
     ]
    }
   ],
   "source": [
    "cleaned_pos_tag = remove_noise(tokens,stop_words)\n",
    "\n",
    "print(cleaned_pos_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "744fcbf1-7017-474c-bc73-056d307d5ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sentence', 'NN'), ('text', 'NN'), ('mining', 'NN'), ('Python', 'NNP'), ('Panasonic', 'NNP'), ('Holdings', 'NNPS')]\n"
     ]
    }
   ],
   "source": [
    "# 指定された品詞を持つtoken, pos_tagを抽出\n",
    "#以下は名詞のみを抽出するもの\n",
    "specified_pos_tag =  [(token, tag) for token, tag in cleaned_pos_tag if tag.startswith('NN')]\n",
    "\n",
    "print(specified_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c33ab85a-a40e-410b-93d7-3f62b48318ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentence', 'text', 'mining', 'Python', 'Panasonic', 'Holdings']\n"
     ]
    }
   ],
   "source": [
    "#tokenのみのリスト\n",
    "\n",
    "specified_tokens = [token for token, _ in specified_pos_tag]\n",
    "print(specified_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34e22876-02a3-45ee-84b1-c5178fce2f59",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pos_tag() got an unexpected keyword argument 'lexicon'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# テキストのトークン化と品詞タギング\u001b[39;00m\n\u001b[1;32m     29\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(text)\n\u001b[0;32m---> 30\u001b[0m pos_tags \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muniversal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlexicon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_vocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#pos_tags = custom_tagger.tag(tokens)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(pos_tags)\n",
      "\u001b[0;31mTypeError\u001b[0m: pos_tag() got an unexpected keyword argument 'lexicon'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# カスタム辞書を読み込む関数\n",
    "def load_custom_dict(file_path):\n",
    "    custom_dict = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            word, pos_tag = line.strip().split()\n",
    "            custom_dict[word] = pos_tag\n",
    "    return custom_dict\n",
    "\n",
    "# カスタム辞書ファイルのパス\n",
    "custom_dict_file = \"userdic/custom_dict.txt\"\n",
    "\n",
    "# カスタム辞書を読み込む\n",
    "custom_vocab = load_custom_dict(custom_dict_file)\n",
    "\n",
    "# NLTKの標準品詞タガーを取得\n",
    "#default_tagger = nltk.data.load('taggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle')\n",
    "#default_tagger = nltk.data.load('/root/nltk_data/taggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle')\n",
    "\n",
    "\n",
    "# カスタムタガーを作成し、標準タガーをバックアップとして指定\n",
    "#custom_tagger = UnigramTagger(model=custom_vocab, backoff=default_tagger)\n",
    "\n",
    "# テキストの定義\n",
    "text = \"This is a sample sentence for text mining in Python provided by Panasonic Holdings.\"\n",
    "\n",
    "\n",
    "# テキストのトークン化と品詞タギング\n",
    "tokens = nltk.word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(tokens, tagset='universal', lexicon=custom_vocab)\n",
    "#pos_tags = custom_tagger.tag(tokens)\n",
    "print(pos_tags)\n",
    "\n",
    "# 指定された品詞を持つトークンを抽出\n",
    "specified_pos_tokens = [token for token, pos_tag in pos_tags if pos_tag in ['NN', 'NNP']]\n",
    "print(specified_pos_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549900cb-ccc0-42be-a7da-555e1c17e296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# テキストをリスト単位で取り込み、リスト単位で形態素解析\n",
    "\n",
    "import MeCab\n",
    "import fitz\n",
    "import re\n",
    "\n",
    "doc = fitz.open(\"data/pana_ar2023j_a4.pdf\") # ドキュメントを開く\n",
    "\n",
    "#ページ単位でテキストをリストに格納\n",
    "#page_textにはインデックスは付与されていない\n",
    "page_text=[]\n",
    "for page in doc:\n",
    "    text = page.get_text() # プレーンテキストを取得\n",
    "    #if text != '' :\n",
    "        #無駄な改行を削除の上で'。'で改行、一文の範囲を明確にする\n",
    "    text = re.sub(r'。','。\\n', re.sub(r'\\n','', text))\n",
    "        #リストに格納\n",
    "    page_text.append(text)\n",
    "#print(page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7831f583-1b61-4b67-936c-c934ea1c5856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_exclusion_list(file_path):\n",
    "    exclusion_list = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            exclusion_list = [line.strip() for line in file]\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    return exclusion_list\n",
    "\n",
    "def noise_eliminator(text):\n",
    "#形態素解析の前に、無駄な記号やヘッダ・フッタ等の文言をテキストから除外\n",
    "\n",
    "    replaced_text = text\n",
    "\n",
    "    #exclusion_list処理前に処理する必要のあるもの\n",
    "    #replaced_text = re.sub(r'[▶]', '', replaced_text)\n",
    "\n",
    "    exclusion_file = \"userdic/exclusion_list.txt\"  # 除外する項目を記載したテキストファイル\n",
    "    exclusion_list = load_exclusion_list(exclusion_file)\n",
    "\n",
    "    for pattern in exclusion_list:\n",
    "        replaced_text = re.sub(pattern, ' ', replaced_text)\n",
    "    \n",
    "    replaced_text = re.sub(r'P\\d+', ' ', replaced_text) \n",
    "        \n",
    "    replaced_text = re.sub(r'[■□▪▫▲△▶▷▸▹▼▽◆◇●〇]', '。\\n', replaced_text)#文頭のこれら記号は箇条書きとして一文とみなす扱い\n",
    "    #replaced_text = re.sub(r'[〇●◇◆□■▶△▲▽▼▫▪▹▶▸]', '', replaced_text)#上記以外は除去\n",
    "    \n",
    "    replaced_text = re.sub(r'[【】（）()\\[\\]]', ' ', replaced_text)\n",
    "    replaced_text = re.sub(r'[\\d\\-]+年度末', '', replaced_text)  # 年度末の除去\n",
    "    replaced_text = re.sub(r'[\\d\\-]+年度', '', replaced_text)  # 年度の除去\n",
    "    replaced_text = re.sub(r'\\d+[年月日]', '', replaced_text)  # 年月日の除去\n",
    "    replaced_text = re.sub(r'\\d+回', '', replaced_text)  # \\d回（取締役の任命回数など）の除去\n",
    "\n",
    "    replaced_text = re.sub(r'[@＠]\\w+', '', replaced_text)  # メンションの除去\n",
    "    replaced_text = re.sub(r'https?://[\\w/;%#\\$\\&\\?\\(\\)~\\.=\\+\\-]+', '', replaced_text)  # リンクの除去\n",
    "    #replaced_text = re.sub(r'\\d+\\.*\\d*', '', replaced_text) #　数字を除去\n",
    "    #replaced_text = text.lower() #　すべて小文字に変換\n",
    "\n",
    "    return replaced_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979ab3e-f2f0-49be-a1f8-a7eacf758c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#形態素解析関数（入力：テキスト、出力：リスト）\n",
    "\n",
    "def mecab_tokenizer(text):\n",
    "\n",
    "    path1 = \"-Ochasen -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
    "    path2 = \" -u /work_dir/userdic/user.dic\"\n",
    "    mecab = MeCab.Tagger(path1+path2)\n",
    "   \n",
    "    parsed_lines = mecab.parse(text).split(\"\\n\")[:-2]\n",
    "    #[:-2]で文末の'EOS',''の取り込みを抑制\n",
    "    #print(parsed_lines)\n",
    "    \n",
    "    #形態素の取得（リスト形式）\n",
    "    #原形を取得\n",
    "    token_list = [l.split(\"\\t\")[2] for l in parsed_lines]\n",
    "    #表層形を確認したい場合\n",
    "    #token_list = [l.split('\\t')[0] for l in parsed_lines]\n",
    "    #print(token_list)\n",
    "    \n",
    "    #品詞区分による絞り込み\n",
    "    #--------------------------------------------\n",
    "    #Chasenの品詞区分を個別に指定する場合\n",
    "    \n",
    "    #品詞区分の取得（リスト形式）\n",
    "    pos = [l.split('\\t')[3] for l in parsed_lines]\n",
    "\n",
    "    #抽出する品詞区分の定義（完全一致で指定）\n",
    "    target_pos = ['名詞-一般',\n",
    "                  '名詞-固有名詞-一般',\n",
    "                  '名詞-固有名詞-人名-一般',\n",
    "                  '名詞-固有名詞-人名-姓',\n",
    "                  '名詞-固有名詞-人名-名',\n",
    "                  '名詞-固有名詞-組織',\n",
    "                  '名詞-固有名詞-地域-一般',\n",
    "                  '名詞-固有名詞-地域-国',\n",
    "                  '名詞-代名詞-一般',\n",
    "                  '名詞-代名詞-縮約',\n",
    "                  '名詞-副詞可能',\n",
    "                  '名詞-サ変接続',\n",
    "                  '名詞-形容動詞語幹',\n",
    "                  '名詞-ナイ形容詞語幹'\n",
    "                 ]\n",
    "    #--------------------------------------------\n",
    "    #Chasenの品詞区分の大項目のみで絞り込む場合\n",
    "    #品詞区分の取得（リスト形式）\n",
    "    #pos = [l.split('\\t')[3].split(\"-\")[0]  for l in parsed_lines]\n",
    "    \n",
    "    #絞り込む品詞区分の定義\n",
    "    #target_pos = ['名詞']\n",
    "    \n",
    "    #--------------------------------------------\n",
    "    #形態素と品詞区分のリストをペアにしてタプルリスト化、該当する品詞区分の形態素のみリストに出力\n",
    "    token_list = [t for t, p in zip(token_list, pos) if p in target_pos]\n",
    "    \n",
    "    #stopwordsの指定\n",
    "    stopwords = []\n",
    "\n",
    "    with open(\"userdic/Japanese.txt\",\"r\") as f:\n",
    "        stopwords1 = f.read().split(\"\\n\")\n",
    "   #stopwordsを直接反映したい場合は以下のリストに記載\n",
    "    stopwords2 =[\"以下\",\"ため\",\"当社\",\"当行\",\"場合\",\"影響\",\"可能性\",\n",
    "            \"状況\",\"グループ\",\"こと\",\"平成\",\"令和\",\"年\",\"月\",\"期\",\"当\",\"他\",\n",
    "            \"等\",\"お\",\"これ\",\"%\",\"以上\",\"もの\",\"株式会社\",\n",
    "            \"もの\",\"とも\",\"ある\",\"よる\",\"的\",\"化\",\"お呼び\",\n",
    "            \"CEO\",\"会長\",\"社長\",\"副社長\",\"専務\",\"役員\",\"常務\",\"代表社員\"\n",
    "            \"代表取締役会長\",\"代表取締役社長\",\"代表取締役\",\"常務取締役\",\"社外取締役\",\"取締役会長\",\"取締役社長\",\n",
    "            \"代表執行役員\",\"専務執行役員\",\"常務執行役員\",\"執行役員\",\n",
    "            \"取締役\",\"取締役会\",\"監査役\",\"監査役会\",\"議長\",\"所長\",\n",
    "            \"担当\",\"100％\",\"100%\",\"株\",\"データ\",\"男性\",\"チーフ\",\"オフィサー\",\"CFO\",\n",
    "            \"入社\",\"入所\",\"就任\",\"選任\",\"指名\",\"任命\",\"解任\",\"諮問\",\"答申\",\"ご参照\",\"全体\",\n",
    "            \"*\"]\n",
    "    stopwords.extend(stopwords1)\n",
    "    stopwords.extend(stopwords2)\n",
    "    # 元の順序を保持しつつ重複を除去（# Python 3.7以降）\n",
    "    from collections import OrderedDict\n",
    "    stopwords = list(OrderedDict.fromkeys(stopwords))\n",
    "    \n",
    "    # stopwordsの除去\n",
    "    token_list = [t for t in token_list if t  not in stopwords]\n",
    "    \n",
    "    # ひらがなのみの単語を除く\n",
    "    kana_re = re.compile(\"^[ぁ-ゖ]+$\")\n",
    "    token_list = [t for t in token_list if not kana_re.match(t)]\n",
    "\n",
    "    # アルファベット1文字のみの形態素を除外\n",
    "    alphabet_re = re.compile(\"^[a-zA-Z]$\")\n",
    "    token_list = [t for t in token_list if not alphabet_re.match(t)]\n",
    "\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d22320-cc30-41c1-9d96-8625a929d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ページインデックス付きで、形態素解析結果をページ単位で格納\n",
    "tokenized_page_text = [(index, mecab_tokenizer(noise_eliminator(item)))for index, item in enumerate(page_text)]\n",
    "#print(page_text[1])\n",
    "#print()\n",
    "#print(tokenized_page_text)\n",
    "\n",
    "#ページインデックスのないもの\n",
    "#tokenized_page_text_noindex = [sublist for _, sublist in tokenized_page_text]\n",
    "#print (tokenized_page_text_noindex)\n",
    "#ページ単位の形態素解析結果を、一つの要素にマージしたもの\n",
    "#merged_tokenized_list = [item for _, sublist in tokenized_page_text for item in sublist]\n",
    "#print (merged_tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76617eaf-9890-4e8e-89f0-e1812b8e5326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#形態素を別途用意した対比リストと比較し、対比リスト内の単語が含まれる要素のみを残す\n",
    "# 対比リストを読込\n",
    "with open(\"data/CGC_Term.txt\", \"r\") as Ref_term:\n",
    "        Ref_list = list(set(Ref_term.read().split(\"\\n\")))\n",
    "#print(Ref_list)\n",
    "\n",
    "#対比リストの単語が含まれるページの形態素リスト\n",
    "Extracted_tokenized_page_text = [(index, sublist) for index, sublist in tokenized_page_text if any(item in Ref_list for item in sublist)]\n",
    "#該当ページのインデックスリスト\n",
    "Extracted_tokenized_page_index = [index for index, sublist in tokenized_page_text if any(item in Ref_list for item in sublist)]\n",
    "#形態素解析前のpage_textの該当ページ部分\n",
    "Extracted_page_text = [page_text[i] for i in Extracted_tokenized_page_index]\n",
    "\n",
    "print(Extracted_tokenized_page_index)\n",
    "#print(Extracted_tokenized_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbc9937-384a-47a9-8837-b62ea10d5db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#マニュアルで抽出ページを指定\n",
    "#man_page_index =[2, 3, 4, 6, 12, 13, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 36, 41, 42, 43, 45, 46, 48, 49, 50, 51, 53, 55]\n",
    "man_page_index = [2, 3, 4, 6, 12, 13, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 36, 42, 49, 50, 51, 53, 55]\n",
    "#man_page_index = [2, 5, 14, 19, 22, 25, 40, 48, 49, 52]\n",
    "\n",
    "#抽出ページ単位の形態素リスト\n",
    "Extracted_tokenized_page_text = [(index, sublist) for index, sublist in tokenized_page_text if index in man_page_index]\n",
    "#該当ページのインデックスリスト\n",
    "Extracted_tokenized_page_index = [index for index, sublist in tokenized_page_text if index in man_page_index]\n",
    "#形態素解析前の抽出ページ単位テキスト\n",
    "Extracted_page_text = [page_text[i] for i in man_page_index]\n",
    "#特定のページのみを直接指定するなら、\n",
    "#Extracted_page_text = [page_text[5]]\n",
    "\n",
    "#print(Extracted_tokenized_page_index)\n",
    "#print(Extracted_tokenized_page_text[2])\n",
    "#print(Extracted_page_text[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95cb7ff-4b2d-4cb2-83d8-b9a889ada289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#抽出されたページのサブリストを一つのリストにまとめる\n",
    "Merged_Extracted_tokenized_list = [item for _, sublist in Extracted_tokenized_page_text for item in sublist]\n",
    "#print(merged_Extracted_tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5fb12-021d-4239-91d4-eef74fca47a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "#辞書形式で単語をカウント\n",
    "counter = Counter(Merged_Extracted_tokenized_list)\n",
    "#for word, count in counter.most_common(500):\n",
    "#    print('%s : %s' % (word, count))\n",
    "\n",
    "# 単語、件数をDataFrameに格納\n",
    "count_df = pd.DataFrame(list(counter.items()), columns=['単語', '件数'])\n",
    "# DataFrameを件数でソート\n",
    "count_df = count_df.sort_values(by='件数', ascending=False)\n",
    "#ファイル名を指定\n",
    "\n",
    "# 現在の日付と時刻を取得\n",
    "now = datetime.datetime.now()\n",
    "# 年月日と時刻の文字列を生成\n",
    "date_time_string = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "# ファイル名を生成\n",
    "file_name = f\"data/Word_list_{date_time_string}.csv\"\n",
    "# 結果をCSVファイルに出力\n",
    "count_df.to_csv(file_name, encoding=\"utf_8_sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9501da-d0e2-4799-a528-bbe39148a6eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#以降、共起分析\n",
    "\n",
    "#Extracted_page_text（抽出ページ単位のリスト）を、要素一つのリストに集約\n",
    "Merged_Extracted_page_text = \"\\n\\n\".join(Extracted_page_text)\n",
    "#print(noise_eliminator(Merged_Extracted_page_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf12c9-8e4f-415b-a295-8bbbeebc6dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#集約したテキストを「。」で区切り、形態素解析（共起分析用）\n",
    "sentences = [mecab_tokenizer(sentence) for sentence in re.split(\"。\", noise_eliminator(Merged_Extracted_page_text))]\n",
    "#sentences = [mecab_tokenizer(sentence) for sentence in re.split(\"。|\\n\", noise_eliminator(Merged_Extracted_page_text))]\n",
    "\n",
    "#print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80cdbcd-e6ea-482f-a914-4b8ad0f4c12e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#各文中の、形態素組み合わせを作る\n",
    "sentences_combs = [list(itertools.combinations(sentence,2)) for sentence in sentences]\n",
    "#print(sentences_combs[0])\n",
    "\n",
    "#組み合わせた2つの形態素の並びをソート\n",
    "words_combs = [[tuple(sorted(words)) for words in sentence] for sentence in sentences_combs]\n",
    "#print(words_combs[0][:30])\n",
    "\n",
    "target_combs = []\n",
    "for words_comb in words_combs:\n",
    "    target_combs.extend(words_comb)\n",
    "#print(target_combs[:30])\n",
    "\n",
    "ct = collections.Counter(target_combs)\n",
    "#print(ct)\n",
    "\n",
    "#import pandas as pd\n",
    "df = pd.DataFrame([{\"1番目\" : i[0][0], \"2番目\": i[0][1], \"count\":i[1]} for i in ct.most_common()])\n",
    "df.head(30)\n",
    "\n",
    "# 現在の日付と時刻を取得\n",
    "#now = datetime.datetime.now()\n",
    "# 年月日と時刻の文字列を生成\n",
    "#date_time_string = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "# ファイル名を生成\n",
    "# date_timeはWordlistと同じにしておく\n",
    "file_name_comb = f\"data/co_count_result_{date_time_string}.csv\"\n",
    "df.to_csv(file_name_comb, encoding=\"utf_8_sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052c3452-bfae-4ca3-bb67-1360e5758479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ネットワーク分析の下準備\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import girvan_newman\n",
    "from pyvis.network import Network\n",
    "import random\n",
    "#import matplotlib\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.rcParams['font.family'] = 'IPAexGothic'  # 使用するフォントを指定（例：IPAexGothic）\n",
    "#import japanize_matplotlib\n",
    "#japanize_matplotlib.japanize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2255f08-a86f-440c-b558-7931490f52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# 分析対象とする共起単語の組み合わせ数（ノード数）を指定\n",
    "analyzed_nodes=1000\n",
    "limited_df=df.head(analyzed_nodes)\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872449a-51e2-4549-b329-fb581b279a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameからネットワークを作成\n",
    "G = nx.from_pandas_edgelist(limited_df, '1番目', '2番目', ['count'])\n",
    "#print(G)\n",
    "\n",
    "# 各ノードの中心性を計算\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "#katz_centrality = nx.katz_centrality(G)\n",
    "\n",
    "# Girvan-Newmanアルゴリズムでコミュニティに分割\n",
    "comp = girvan_newman(G)\n",
    "communities = tuple(sorted(c) for c in next(comp))\n",
    "\n",
    "# 各ノードがどのコミュニティに属するかを記録\n",
    "community_map = {}\n",
    "for i, community in enumerate(communities):\n",
    "    for node in community:\n",
    "        community_map[node] = i\n",
    "\n",
    "# 中心性を新しいデータフレームに格納\n",
    "centrality_df = pd.DataFrame({\n",
    "    'Node': list(G.nodes()),\n",
    "    'Degree Centrality': [degree_centrality[node] for node in G.nodes()],\n",
    "    'Betweenness Centrality': [betweenness_centrality[node] for node in G.nodes()],\n",
    "    'Closeness Centrality': [closeness_centrality[node] for node in G.nodes()],\n",
    "    'Eigenvector Centrality': [eigenvector_centrality[node] for node in G.nodes()],\n",
    "#    'Katz Centrality': [katz_centrality[node] for node in G.nodes()],\n",
    "    'Community': [community_map[node] for node in G.nodes()]  # コミュニティ情報を追加\n",
    "})\n",
    "#print(centrality_df)\n",
    "\n",
    "file_name_comb = f\"data/Centrality_{date_time_string}.csv\"\n",
    "centrality_df.to_csv(file_name_comb, encoding=\"utf_8_sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ed190-926d-4560-b88e-848f99db9cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ネットワーク図の描画\n",
    "#ここではpyvisを使用\n",
    "\n",
    "class ColoredObject:\n",
    "    def __init__(self, color):\n",
    "        self.color = color\n",
    "# 12種類の色を用意\n",
    "colors = [\"#CBCCCD\", \"red\", \"blue\", \"green\", \"#A2CEF6\", \"magenta\", \"brown\",  \"yellow\", \"orange\", \"purple\", \"#EDABE5\", \"black\" ]\n",
    "# 色の順番を追跡するためのインデックス\n",
    "color_index = 0\n",
    "# 色を順番に取得する関数\n",
    "def choose_color():\n",
    "    global color_index\n",
    "    color = colors[color_index]\n",
    "    color_index = (color_index + 1) % len(colors)  # インデックスを更新し、13番目の場合は最初の色に戻る\n",
    "    return color\n",
    "\n",
    "def kyoki_word_network(df):\n",
    "\n",
    "    #Girvan-Newmanアルゴリズムによるコミュニティ分割（networkxの機能）\n",
    "    G = nx.from_pandas_edgelist(df, '1番目', '2番目', ['count'])\n",
    "    comp = girvan_newman(G)\n",
    "    communities = tuple(sorted(c) for c in next(comp))\n",
    "\n",
    "    #各コミュニティに色を割り当てる\n",
    "    color_map = {}\n",
    "    for i, community in enumerate(communities):\n",
    "        color = choose_color() #\"#{:06x}\".format(random.randint(0, 0xFFFFFF))  # ランダムな色を生成\n",
    "        for node in community:\n",
    "            color_map[node] = color\n",
    "    \n",
    "    \n",
    "    got_net = Network(height=\"1000px\", width=\"95%\", bgcolor=\"#FFFFFF\", font_color=\"black\", notebook=True)\n",
    "\n",
    "    got_net.force_atlas_2based()\n",
    "    got_data = df\n",
    "\n",
    "    sources = got_data['1番目']\n",
    "    targets = got_data['2番目']\n",
    "    weights = got_data['count']\n",
    "\n",
    "    edge_data = zip(sources, targets, weights)\n",
    "\n",
    "    for e in edge_data:\n",
    "        src = e[0]\n",
    "        dst = e[1]\n",
    "        w = e[2]\n",
    "        got_net.add_node(src, src, title=src, color=color_map.get(src, None))\n",
    "        got_net.add_node(dst, dst, title=dst, color=color_map.get(dst, None))\n",
    "        got_net.add_edge(src, dst, value=w)\n",
    "\n",
    "    # コミュニティごとにノードの色を設定\n",
    "    for node, color in color_map.items():\n",
    "        got_net.add_node(node, color=color)\n",
    "\n",
    "    neighbor_map = got_net.get_adj_list()\n",
    "\n",
    "    for node in got_net.nodes:\n",
    "        node[\"title\"] += \" Neighbors:<br>\" + \"<br>\".join(neighbor_map[node[\"id\"]])\n",
    "        node[\"value\"] = len(neighbor_map[node[\"id\"]])\n",
    "\n",
    "    got_net.show_buttons(filter_=['physics'])\n",
    "    return got_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae2800-1f2b-4ab9-bc78-f21b3120bd35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "got_net = kyoki_word_network(limited_df)\n",
    "got_net.show(\"kyoki_\"+str(analyzed_nodes)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524fff0-4afe-40ad-9e00-d94e0ea78bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d54319-1aa3-4ef9-83ef-22304d5e85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "custom_dict = {\"word1\": \"NN\", \"word2\": \"VB\"}\n",
    "\n",
    "sentence = \"Your sentence here\"\n",
    "tokens = word_tokenize(sentence)\n",
    "tagged_tokens = nltk.pos_tag(tokens, tagset='universal', lexicon=custom_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612b536-6198-4743-9907-26e8bdbd7858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import PerceptronTagger\n",
    "\n",
    "# ユーザー辞書を読み込む関数\n",
    "def load_custom_dict(file_path):\n",
    "    custom_dict = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            word, pos_tag = line.strip().split()\n",
    "            custom_dict[word] = pos_tag\n",
    "    return custom_dict\n",
    "\n",
    "# カスタム辞書ファイルのパス\n",
    "custom_dict_file = \"userdic/custom_dict.txt\"\n",
    "\n",
    "# カスタム辞書を読み込む\n",
    "custom_vocab = load_custom_dict(custom_dict_file)\n",
    "\n",
    "# PerceptronTaggerの初期化\n",
    "tagger = PerceptronTagger(load=False)\n",
    "\n",
    "# ユーザー辞書をタガーに追加\n",
    "tagger.classes = {}\n",
    "tagger.model.classes = tagger.classes\n",
    "tagger.tagdict.update(custom_vocab)\n",
    "\n",
    "# テキストの定義\n",
    "text = \"This is a sample sentence for text mining in Pytho provided by Panasonic.\"\n",
    "\n",
    "# テキストのトークン化と品詞タギング\n",
    "tokens = nltk.word_tokenize(text)\n",
    "pos_tags = tagger.tag(tokens)\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc6d91-a70f-429d-a948-c51307a5ccd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
