{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549900cb-ccc0-42be-a7da-555e1c17e296",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#構文解析・共起語抽出\n",
    "#辞書：NEOlogd＋ユーザー辞書\n",
    "#ストップワードを除去\n",
    "\n",
    "import MeCab\n",
    "\n",
    "#インプットファイル\n",
    "f= open('data/TCFD_Guidance_3.0_J_and_Implementing_Guidance.txt', encoding=\"utf-8\")\n",
    "text = f.read()\n",
    "f.close\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7831f583-1b61-4b67-936c-c934ea1c5856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#初期処理\n",
    "import re\n",
    "\n",
    "replaced_text = text\n",
    "replaced_text = re.sub(r'[【】]', ' ', replaced_text)       # 【】の除去\n",
    "replaced_text = re.sub(r'[（）()]', ' ', replaced_text)     # （）の除去\n",
    "replaced_text = re.sub(r'[［］\\[\\]]', ' ', replaced_text)   # ［］の除去\n",
    "replaced_text = re.sub(r'[〇●◇◆□■△▲▽▼▫▪▹▶▸]', ' ', replaced_text)   # 〇・□等の除去\n",
    "replaced_text = re.sub(r'Panasonic Holdings   統合報告書2023 \\d+', ' ', replaced_text)   # ヘッダ・フッタ等の除去\n",
    "replaced_text = re.sub(r'\\d+ Panasonic Holdings   統合報告書2023', ' ', replaced_text)   # ヘッダ・フッタ等の除去\n",
    "replaced_text = re.sub(r'パナソニックグループについて　￨　トップメッセージ　￨　セグメント別戦略　￨　テーマ別戦略　￨　コーポレート・ガバナンス　￨　企業データ', ' ', replaced_text)   # ヘッダ・フッタ等の除去\n",
    "replaced_text = re.sub(r'[@＠]\\w+', '', replaced_text)  # メンションの除去\n",
    "replaced_text = re.sub(r'[\\d\\-]+年度末', '', replaced_text)  # 年度末の除去\n",
    "replaced_text = re.sub(r'[\\d\\-]+年度', '', replaced_text)  # 年度の除去\n",
    "replaced_text = re.sub(r'\\d+[年月日]', '', replaced_text)  # 年月日の除去\n",
    "replaced_text = re.sub(r'https?://[\\w/;%#\\$\\&\\?\\(\\)~\\.=\\+\\-]+', '', replaced_text)  # リンクの除去\n",
    "replaced_text = re.sub(r'\\d+回', '', replaced_text)  # \\d回（多分取締役の回数）の除去\n",
    "#replaced_text = re.sub(r'\\d+\\.*\\d*', '', replaced_text) #　数字を除去\n",
    "#replaced_text = text.lower()\n",
    "\n",
    "print(replaced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e979ab3e-f2f0-49be-a1f8-a7eacf758c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#形態素解析関数（入力：テキスト、出力：リスト）\n",
    "\n",
    "def mecab_tokenizer(text):\n",
    "\n",
    "    path1 = \"-Ochasen -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
    "    path2 = \" -u /work_dir/userdic/user.dic\"\n",
    "    mecab = MeCab.Tagger(path1+path2)\n",
    "   \n",
    "    parsed_lines = mecab.parse(text).split(\"\\n\")[:-2]\n",
    "    #[:-2]で文末の'EOS',''の取り込みを抑制\n",
    "    #print(parsed_lines)\n",
    "    \n",
    "    #形態素の取得（リスト形式）\n",
    "    #原形を取得\n",
    "    token_list = [l.split(\"\\t\")[2] for l in parsed_lines]\n",
    "    #表層形を確認したい場合\n",
    "    #token_list = [l.split('\\t')[0] for l in parsed_lines]\n",
    "    #print(token_list)\n",
    "    \n",
    "    #品詞区分による絞り込み\n",
    "    #--------------------------------------------\n",
    "    #Chasenの品詞区分を個別に指定する場合\n",
    "    \n",
    "    #品詞区分の取得（リスト形式）\n",
    "    pos = [l.split('\\t')[3] for l in parsed_lines]\n",
    "\n",
    "    #抽出する品詞区分の定義（完全一致で指定）\n",
    "    target_pos = ['名詞-一般',\n",
    "                  '名詞-固有名詞-一般',\n",
    "                  '名詞-固有名詞-人名-一般',\n",
    "                  '名詞-固有名詞-人名-姓',\n",
    "                  '名詞-固有名詞-人名-名',\n",
    "                  '名詞-固有名詞-組織',\n",
    "                  '名詞-固有名詞-地域-一般',\n",
    "                  '名詞-固有名詞-地域-国',\n",
    "                  '名詞-代名詞-一般',\n",
    "                  '名詞-代名詞-縮約',\n",
    "                  '名詞-副詞可能',\n",
    "                  '名詞-サ変接続',\n",
    "                  '名詞-形容動詞語幹',\n",
    "                  '名詞-ナイ形容詞語幹'\n",
    "                 ]\n",
    "    #--------------------------------------------\n",
    "    #Chasenの品詞区分の大項目のみで絞り込む場合\n",
    "    #品詞区分の取得（リスト形式）\n",
    "    #pos = [l.split('\\t')[3].split(\"-\")[0]  for l in parsed_lines]\n",
    "    \n",
    "    #絞り込む品詞区分の定義\n",
    "    #target_pos = ['名詞']\n",
    "    \n",
    "    #--------------------------------------------\n",
    "    #形態素と品詞区分のリストをペアにしてタプルリスト化、該当する品詞区分の形態素のみリストに出力\n",
    "    token_list = [t for t, p in zip(token_list, pos) if p in target_pos]\n",
    "    \n",
    "    #stopwordsの指定\n",
    "    with open(\"userdic/Japanese.txt\",\"r\") as f:\n",
    "        stopwords1 = f.read().split(\"\\n\")\n",
    "        #stopwordsを直接反映したい場合は以下のリストに記載\n",
    "        stopwords2 =[\"以下\",\"ため\",\"当社\",\"当行\",\"場合\",\"影響\",\"可能性\",\n",
    "            \"状況\",\"グループ\",\"こと\",\"平成\",\"令和\",\"年\",\"月\",\"期\",\"当\",\"他\",\n",
    "            \"等\",\"お\",\"これ\",\"%\",\"以上\",\"もの\",\"株式会社\",\n",
    "            \"もの\",\"とも\",\"ある\",\"よる\",\"的\",\"化\",\"お呼び\",\n",
    "            \"CEO\",\"会長\",\"社長\",\"副社長\",\"専務\",\"役員\",\"常務\",\"代表社員\"\n",
    "            \"代表取締役会長\",\"代表取締役社長\",\"代表取締役\",\"常務取締役\",\"社外取締役\",\"取締役会長\",\"取締役社長\",\n",
    "            \"代表執行役員\",\"専務執行役員\",\"常務執行役員\",\"執行役員\",\n",
    "            \"取締役\",\"取締役会\",\"監査役\",\"監査役会\",\"議長\",\"所長\",\n",
    "            \"担当\",\"100％\",\"100%\",\"株\",\"データ\",\"男性\",\"チーフ\",\"オフィサー\",\"CFO\",\n",
    "            \"入社\",\"入所\",\"就任\",\"選任\",\"指名\",\"任命\",\"解任\",\"諮問\",\"答申\",\n",
    "            \"*\"]\n",
    "        stopwords = list(set(stopwords1+stopwords2))\n",
    "    \n",
    "    # stopwordsの除去\n",
    "    token_list = [t for t in token_list if t  not in stopwords]\n",
    "    \n",
    "    # ひらがなのみの単語を除く\n",
    "    kana_re = re.compile(\"^[ぁ-ゖ]+$\")\n",
    "    token_list = [t for t in token_list if not kana_re.match(t)]\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76617eaf-9890-4e8e-89f0-e1812b8e5326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_text = mecab_tokenizer(replaced_text)\n",
    "#print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53c5fb12-021d-4239-91d4-eef74fca47a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "#辞書形式で単語をカウント\n",
    "counter = Counter(tokenized_text)\n",
    "#for word, count in counter.most_common(500):\n",
    "#    print('%s : %s' % (word, count))\n",
    "\n",
    "# 単語、件数をDataFrameに格納\n",
    "count_df = pd.DataFrame(list(counter.items()), columns=['単語', '件数'])\n",
    "# DataFrameを件数でソート\n",
    "count_df = count_df.sort_values(by='件数', ascending=False)\n",
    "# 結果をCSVファイルに出力\n",
    "count_df.to_csv('Word_list.csv', encoding=\"utf_8_sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fdf12c9-8e4f-415b-a295-8bbbeebc6dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import itertools\n",
    "#共起の範囲を一つの文中に限定するため、元のテキストを”。”単位で分割し、文単位で形態素に分解・格納\n",
    "sentences = [mecab_tokenizer(sentence) for sentence in replaced_text.split(\"。\")]\n",
    "#print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b80cdbcd-e6ea-482f-a914-4b8ad0f4c12e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#各分の形態素2つの組み合わせを作る\n",
    "sentences_combs = [list(itertools.combinations(sentence,2)) for sentence in sentences]\n",
    "#print(sentences_combs[0])\n",
    "\n",
    "#組み合わせた2つの形態素の並びをソート\n",
    "words_combs = [[tuple(sorted(words)) for words in sentence] for sentence in sentences_combs]\n",
    "#print(words_combs[0][:30])\n",
    "\n",
    "target_combs = []\n",
    "for words_comb in words_combs:\n",
    "    target_combs.extend(words_comb)\n",
    "#print(target_combs[:30])\n",
    "    \n",
    "\n",
    "ct = collections.Counter(target_combs)\n",
    "#print(ct)\n",
    "\n",
    "#import pandas as pd\n",
    "df = pd.DataFrame([{\"1番目\" : i[0][0], \"2番目\": i[0][1], \"count\":i[1]} for i in ct.most_common()])\n",
    "df.head(30)\n",
    "df.to_csv('co_count_result.csv', encoding=\"utf_8_sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "788ed190-926d-4560-b88e-848f99db9cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kyoki_word_network(df):\n",
    "    from pyvis.network import Network\n",
    "    import pandas as pd\n",
    "\n",
    "    \n",
    "    got_net = Network(height=\"1000px\", width=\"95%\", bgcolor=\"#FFFFFF\", font_color=\"black\", notebook=True)\n",
    "\n",
    "    got_net.force_atlas_2based()\n",
    "    got_data = df[:600]\n",
    "\n",
    "    sources = got_data['1番目']#count\n",
    "    targets = got_data['2番目']#first\n",
    "    weights = got_data['count']#second\n",
    "\n",
    "    edge_data = zip(sources, targets, weights)\n",
    "\n",
    "    for e in edge_data:\n",
    "        src = e[0]\n",
    "        dst = e[1]\n",
    "        w = e[2]\n",
    "\n",
    "        got_net.add_node(src, src, title=src)\n",
    "        got_net.add_node(dst, dst, title=dst)\n",
    "        got_net.add_edge(src, dst, value=w)\n",
    "\n",
    "    neighbor_map = got_net.get_adj_list()\n",
    "\n",
    "    for node in got_net.nodes:\n",
    "        node[\"title\"] += \" Neighbors:<br>\" + \"<br>\".join(neighbor_map[node[\"id\"]])\n",
    "        node[\"value\"] = len(neighbor_map[node[\"id\"]])\n",
    "\n",
    "    got_net.show_buttons(filter_=['physics'])\n",
    "    return got_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49ae2800-1f2b-4ab9-bc78-f21b3120bd35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"95%\"\n",
       "            height=\"1000px\"\n",
       "            src=\"kyoki.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f2527622040>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "got_net = kyoki_word_network(df)\n",
    "got_net.show(\"kyoki.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699c6fd-8f4e-4c57-af16-a7a32dd275af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
