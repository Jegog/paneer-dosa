{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "549900cb-ccc0-42be-a7da-555e1c17e296",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Executive Summary                                                      1 Ⅰ はじめに 5 (１) 本ガイドラインの位置付け 5 (２) 本ガイドラインの想定利用者と期待される活用方法 12 (３) 本ガイドラインにおける「知財・無形資産の投資・活用」のスコープ 15 Ⅱ 日本における知財・無形資産の投資・活用の現状と課題 16 (１) 企業における戦略構築・ガバナンス等の課題 17 (２) 投資家・金融機関における中長期的視点の欠如 20 (３) ジャパン・パッシングの問題 20 (４) 企業と投資家・金融機関の間の思考構造のギャップ 22 Ⅲ 企業価値を顕在化するコミュニケーション・フレームワーク 25 (１) 知財・無形資産の投資・活用を企業変革につなげる「ストーリー」の構築と磨き上げ 27 (２) 知財・無形資産の投資・活用と事業価値をつなぐ因果パスの企図・実装 29 (３) 経営指標（ROIC 等）と知財・無形資産の投資・活用戦略の紐付け 32 Ⅳ 企業に求められる知財・無形資産の投資・活用戦略の構築・開示・発信 34 (１) 企業が意識すべき投資家や金融機関が重視する視点 35 ①知財・無形資産を「価格決定力」「ゲームチェンジ」につなげる 35 ②知財・無形資産の投資・活用を「費用」でなく「資産」形成として捉える 37 ③「ロジック/ストーリー」としての説得的な説明 37 ④全社横断的な体制整備とガバナンス構築 37 (２) 企業における知財・無形資産の投資・活用にかかる戦略構築の流れ 38 ①自社の現状のビジネスモデルと強みとなる知財・無形資産の把握・分析 38 ②知財・無形資産を活用した持続的成長に繋がるビジネスモデルの検討 39 ③競争優位を支える知財・無形資産の維持・強化に向けた戦略の構築 41 ④スタートアップに対する経営資源提供を通じた価値協創能力の構築 41 ⑤サプライチェーンとのパートナーシップにおける外部の知財・無形資産の有効活用 47 (３) 多様な投資家・金融機関に対する開示・発信・対話の実行 48 ①定性的・定量的な説明（KPI 等含む） 48 ②様々な媒体を通じた戦略の開示・発信 50 ③セグメント単位の開示・発信 51 ④投資家との双方向の対話の実践 52 (４) 知財・無形資産を経営変革や企業価値に繋ぐガバナンスの実践 54 ①全社横断的な体制の構築 54 ②取締役会によるガバナンス 55 ③社内における連携体制・人材育成 56 Ⅴ 投資家や金融機関等に期待される役割 58 (１) アセット・オーナーに期待される役割 59 (２) 投資家（アセット・マネージャー）に期待される役割 60 ①アクティブ運用における期待行動 61 ②パッシブ運用における期待行動 61 ③クオンツ運用における期待行動 62 (３)アナリストに期待される役割 62 (４) 金融機関（間接金融機関）に期待される役割 62 (５) ベンチャー・キャピタル（VC）に期待される役割 63 (６) 知財・無形資産の専門調査・コンサルティング会社等に期待される役割 64 Ⅵ おわりに（今後に向けた課題） 65 参考資料集 図表・コラム・事例 \n",
      "\n",
      " 1  Executive Summary １．背景 • 近年、企業価値の源泉は知的財産（知財）・無形資産へと変化しており、それらは企業における競争力の源泉として、そして、サステナビリティ・トランスフォーメーション（SX）を実現する上で、重要な経営資源となっている。\n",
      "既に米国企業では、企業価値に占める無形資産の割合が過半を越えており、日本企業においても、企業価値向上の実現に向けては、大胆な知財・無形資産の投資・活用の実行が不可欠である。\n",
      " • しかしながら、2022 年7 月時点で東証株価指数（TOPIX）500 構成銘柄のうちPBR1 倍割れの企業の比率は43%であり、米S&P500・欧州ストックス600 よりも極めて多い。\n",
      "また、海外投資家による日本株離れも指摘されている。\n",
      "今後、日本企業が国内外の資本・金融市場において十分に評価されるためには、知財・無形資産の投資・活用戦略やその開示を含む、企業価値向上に向けた取り組みが一層必要になると考えられる。\n",
      " • こうした状況を踏まえ、企業及び投資家・金融機関は、相互の価値協創（対話・エンゲージメント等）を通じて、思考構造のギャップを埋め、企業価値の創造に資する知財・無形資産の投資・活用を実行することが求められる。\n",
      " • そこで本ガイドラインでは、企業、投資家・金融機関に求められる「５つのプリンシプル（原則）」、企業に求められる取組である「７つのアクション」及び、企業と投資家・金融機関における価値協創を更に加速すべく、両者における共通の枠組みである「コミュニケーション・フレームワーク」を提示している。\n",
      " • 本ガイドラインの狙いは、知財・無形資産の投資・活用にかかる企業と投資家・金融機関の思考構造のギャップを埋め、企業における資本・金融市場からの理解やサポートの獲得、企業価値の向上、資本・金融市場の力を活かした更なる知財・無形資産への投資資金の獲得という好循環を加速化することである。\n",
      "  ２．知財・無形資産ガバナンスガイドラインVer.2.0 の狙いと概要 • 本ガイドラインは、知財・無形資産の投資・活用の促進による、企業価値の向上と投資資金の獲得という好循環を加速化すべく、全体を通底する「プリンシプル（原則）」、企業の「アクション」、企業と投資家・金融機関間の「コミュニケーション・フレームワーク」、を示すものである。\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# テキストをリスト単位で取り込み、リスト単位で形態素解析\n",
    "\n",
    "import MeCab\n",
    "import fitz\n",
    "import re\n",
    "\n",
    "doc = fitz.open(\"知財・無形資産ガバナンスガイドライン_ver2.pdf\") # ドキュメントを開く\n",
    "\n",
    "#ページ単位でテキストをリストに格納\n",
    "page_text=[]\n",
    "for page in doc:\n",
    "    text = page.get_text() # プレーンテキストを取得\n",
    "    if text != '' :\n",
    "        #無駄な改行を削除の上で'。'で改行、一文の範囲を明確にする\n",
    "        text = re.sub(r'。','。\\n', re.sub(r'\\n','', text))\n",
    "        #リストに格納\n",
    "        page_text.append(text)\n",
    "print(page_text[1])\n",
    "print() \n",
    "print(page_text[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7831f583-1b61-4b67-936c-c934ea1c5856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def noise_eliminator(text):\n",
    "#形態素解析の前に、無駄な記号やヘッダ・フッタ等の文言をテキストから除外\n",
    "\n",
    "    replaced_text = text\n",
    "\n",
    "    replaced_text = re.sub(r'Panasonic Holdings   統合報告書2023 \\d+', ' ', replaced_text)   # ヘッダ・フッタ等の除去\n",
    "    replaced_text = re.sub(r'\\d+ Panasonic Holdings   統合報告書2023', ' ', replaced_text)   # ヘッダ・フッタ等の除去\n",
    "    replaced_text = re.sub(r'パナソニックグループについて　￨　トップメッセージ　￨　セグメント別戦略　￨　テーマ別戦略　￨　コーポレート・ガバナンス　￨　企業データ', ' ', replaced_text)   # ヘッダ・フッタ等の除去\n",
    "\n",
    "    replaced_text = re.sub(r'[【】]', ' ', replaced_text)       # 【】の除去\n",
    "    replaced_text = re.sub(r'[（）()]', ' ', replaced_text)     # （）の除去\n",
    "    replaced_text = re.sub(r'[［］\\[\\]]', ' ', replaced_text)   # ［］の除去\n",
    "    replaced_text = re.sub(r'[〇●◇◆□■△▲▽▼▫▪▹▶▸]', ' ', replaced_text)   # 〇・□等の除去\n",
    "\n",
    "    replaced_text = re.sub(r'[\\d\\-]+年度末', '', replaced_text)  # 年度末の除去\n",
    "    replaced_text = re.sub(r'[\\d\\-]+年度', '', replaced_text)  # 年度の除去\n",
    "    replaced_text = re.sub(r'\\d+[年月日]', '', replaced_text)  # 年月日の除去\n",
    "    replaced_text = re.sub(r'\\d+回', '', replaced_text)  # \\d回（取締役の任命回数など）の除去\n",
    "\n",
    "    replaced_text = re.sub(r'[@＠]\\w+', '', replaced_text)  # メンションの除去\n",
    "    replaced_text = re.sub(r'https?://[\\w/;%#\\$\\&\\?\\(\\)~\\.=\\+\\-]+', '', replaced_text)  # リンクの除去\n",
    "    #replaced_text = re.sub(r'\\d+\\.*\\d*', '', replaced_text) #　数字を除去\n",
    "    #replaced_text = text.lower() #　すべて小文字に変換\n",
    "\n",
    "    return replaced_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e979ab3e-f2f0-49be-a1f8-a7eacf758c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#形態素解析関数（入力：テキスト、出力：リスト）\n",
    "\n",
    "def mecab_tokenizer(text):\n",
    "\n",
    "    path1 = \"-Ochasen -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
    "    path2 = \" -u /work_dir/userdic/user.dic\"\n",
    "    mecab = MeCab.Tagger(path1+path2)\n",
    "   \n",
    "    parsed_lines = mecab.parse(text).split(\"\\n\")[:-2]\n",
    "    #[:-2]で文末の'EOS',''の取り込みを抑制\n",
    "    #print(parsed_lines)\n",
    "    \n",
    "    #形態素の取得（リスト形式）\n",
    "    #原形を取得\n",
    "    token_list = [l.split(\"\\t\")[2] for l in parsed_lines]\n",
    "    #表層形を確認したい場合\n",
    "    #token_list = [l.split('\\t')[0] for l in parsed_lines]\n",
    "    #print(token_list)\n",
    "    \n",
    "    #品詞区分による絞り込み\n",
    "    #--------------------------------------------\n",
    "    #Chasenの品詞区分を個別に指定する場合\n",
    "    \n",
    "    #品詞区分の取得（リスト形式）\n",
    "    pos = [l.split('\\t')[3] for l in parsed_lines]\n",
    "\n",
    "    #抽出する品詞区分の定義（完全一致で指定）\n",
    "    target_pos = ['名詞-一般',\n",
    "                  '名詞-固有名詞-一般',\n",
    "                  '名詞-固有名詞-人名-一般',\n",
    "                  '名詞-固有名詞-人名-姓',\n",
    "                  '名詞-固有名詞-人名-名',\n",
    "                  '名詞-固有名詞-組織',\n",
    "                  '名詞-固有名詞-地域-一般',\n",
    "                  '名詞-固有名詞-地域-国',\n",
    "                  '名詞-代名詞-一般',\n",
    "                  '名詞-代名詞-縮約',\n",
    "                  '名詞-副詞可能',\n",
    "                  '名詞-サ変接続',\n",
    "                  '名詞-形容動詞語幹',\n",
    "                  '名詞-ナイ形容詞語幹'\n",
    "                 ]\n",
    "    #--------------------------------------------\n",
    "    #Chasenの品詞区分の大項目のみで絞り込む場合\n",
    "    #品詞区分の取得（リスト形式）\n",
    "    #pos = [l.split('\\t')[3].split(\"-\")[0]  for l in parsed_lines]\n",
    "    \n",
    "    #絞り込む品詞区分の定義\n",
    "    #target_pos = ['名詞']\n",
    "    \n",
    "    #--------------------------------------------\n",
    "    #形態素と品詞区分のリストをペアにしてタプルリスト化、該当する品詞区分の形態素のみリストに出力\n",
    "    token_list = [t for t, p in zip(token_list, pos) if p in target_pos]\n",
    "    \n",
    "    #stopwordsの指定\n",
    "    with open(\"Japanese.txt\",\"r\") as f:\n",
    "        stopwords1 = f.read().split(\"\\n\")\n",
    "        #stopwordsを直接反映したい場合は以下のリストに記載\n",
    "        stopwords2 =[\"以下\",\"ため\",\"当社\",\"当行\",\"場合\",\"影響\",\"可能性\",\n",
    "            \"状況\",\"グループ\",\"こと\",\"平成\",\"令和\",\"年\",\"月\",\"期\",\"当\",\"他\",\n",
    "            \"等\",\"お\",\"これ\",\"%\",\"以上\",\"もの\",\"株式会社\",\n",
    "            \"もの\",\"とも\",\"ある\",\"よる\",\"的\",\"化\",\"お呼び\",\n",
    "            \"CEO\",\"会長\",\"社長\",\"副社長\",\"専務\",\"役員\",\"常務\",\"代表社員\"\n",
    "            \"代表取締役会長\",\"代表取締役社長\",\"代表取締役\",\"常務取締役\",\"社外取締役\",\"取締役会長\",\"取締役社長\",\n",
    "            \"代表執行役員\",\"専務執行役員\",\"常務執行役員\",\"執行役員\",\n",
    "            \"取締役\",\"取締役会\",\"監査役\",\"監査役会\",\"議長\",\"所長\",\n",
    "            \"担当\",\"100％\",\"100%\",\"株\",\"データ\",\"男性\",\"チーフ\",\"オフィサー\",\"CFO\",\n",
    "            \"入社\",\"入所\",\"就任\",\"選任\",\"指名\",\"任命\",\"解任\",\"諮問\",\"答申\",\n",
    "            \"*\"]\n",
    "        stopwords = list(set(stopwords1+stopwords2))\n",
    "    \n",
    "    # stopwordsの除去\n",
    "    token_list = [t for t in token_list if t  not in stopwords]\n",
    "    \n",
    "    # ひらがなのみの単語を除く\n",
    "    kana_re = re.compile(\"^[ぁ-ゖ]+$\")\n",
    "    token_list = [t for t in token_list if not kana_re.match(t)]\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d22320-cc30-41c1-9d96-8625a929d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ページのインデックス付きで、形態素解析結果をページ単位で格納\n",
    "tokenized_page_text = [(index, mecab_tokenizer(noise_eliminator(item))) for index, item in enumerate(page_text)]\n",
    "\n",
    "#インデックスのないもの\n",
    "tokenized_page_text_noindex = [sublist for _, sublist in tokenized_page_text]\n",
    "#全頁マージしたもの\n",
    "merged_tokenized_list = [item for _, sublist in tokenized_page_text for item in sublist]\n",
    "\n",
    "#形態素分解前の全テキスト\n",
    "merged_page_text = \" \".join(page_text)\n",
    "\n",
    "#結果を確認する場合に出力)\n",
    "#print(page_text[1])\n",
    "#print()\n",
    "#print(tokenized_page_text[1])\n",
    "#print (tokenized_page_text_noindex)\n",
    "print (merged_page_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdf04d8-d0be-418f-b558-0db824b8ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53c5fb12-021d-4239-91d4-eef74fca47a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "#辞書形式で単語をカウント\n",
    "counter = Counter(merged_tokenized_list)\n",
    "#for word, count in counter.most_common(500):\n",
    "#    print('%s : %s' % (word, count))\n",
    "\n",
    "# 単語、件数をDataFrameに格納\n",
    "count_df = pd.DataFrame(list(counter.items()), columns=['単語', '件数'])\n",
    "# DataFrameを件数でソート\n",
    "count_df = count_df.sort_values(by='件数', ascending=False)\n",
    "# 結果をCSVファイルに出力\n",
    "count_df.to_csv('CGC_Guideline_v2_Word_list.csv', encoding=\"utf_8_sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fdf12c9-8e4f-415b-a295-8bbbeebc6dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import itertools\n",
    "#共起の範囲を一つの文中に限定するため、元のテキストを”。”単位で分割し、文単位で形態素に分解・格納\n",
    "sentences = [mecab_tokenizer(sentence) for sentence in merged_page_text.split(\"。\")]\n",
    "#print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b80cdbcd-e6ea-482f-a914-4b8ad0f4c12e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#各分の形態素2つの組み合わせを作る\n",
    "sentences_combs = [list(itertools.combinations(sentence,2)) for sentence in sentences]\n",
    "#print(sentences_combs[0])\n",
    "\n",
    "#組み合わせた2つの形態素の並びをソート\n",
    "words_combs = [[tuple(sorted(words)) for words in sentence] for sentence in sentences_combs]\n",
    "#print(words_combs[0][:30])\n",
    "\n",
    "target_combs = []\n",
    "for words_comb in words_combs:\n",
    "    target_combs.extend(words_comb)\n",
    "#print(target_combs[:30])\n",
    "    \n",
    "\n",
    "ct = collections.Counter(target_combs)\n",
    "#print(ct)\n",
    "\n",
    "#import pandas as pd\n",
    "df = pd.DataFrame([{\"1番目\" : i[0][0], \"2番目\": i[0][1], \"count\":i[1]} for i in ct.most_common()])\n",
    "df.head(30)\n",
    "df.to_csv('CGC_Guideline_v2_co_count_result.csv', encoding=\"utf_8_sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "788ed190-926d-4560-b88e-848f99db9cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kyoki_word_network(df):\n",
    "    from pyvis.network import Network\n",
    "    import pandas as pd\n",
    "\n",
    "    \n",
    "    got_net = Network(height=\"1000px\", width=\"95%\", bgcolor=\"#FFFFFF\", font_color=\"black\", notebook=True)\n",
    "\n",
    "    got_net.force_atlas_2based()\n",
    "    got_data = df[:600]\n",
    "\n",
    "    sources = got_data['1番目']#count\n",
    "    targets = got_data['2番目']#first\n",
    "    weights = got_data['count']#second\n",
    "\n",
    "    edge_data = zip(sources, targets, weights)\n",
    "\n",
    "    for e in edge_data:\n",
    "        src = e[0]\n",
    "        dst = e[1]\n",
    "        w = e[2]\n",
    "\n",
    "        got_net.add_node(src, src, title=src)\n",
    "        got_net.add_node(dst, dst, title=dst)\n",
    "        got_net.add_edge(src, dst, value=w)\n",
    "\n",
    "    neighbor_map = got_net.get_adj_list()\n",
    "\n",
    "    for node in got_net.nodes:\n",
    "        node[\"title\"] += \" Neighbors:<br>\" + \"<br>\".join(neighbor_map[node[\"id\"]])\n",
    "        node[\"value\"] = len(neighbor_map[node[\"id\"]])\n",
    "\n",
    "    got_net.show_buttons(filter_=['physics'])\n",
    "    return got_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49ae2800-1f2b-4ab9-bc78-f21b3120bd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"95%\"\n",
       "            height=\"1000px\"\n",
       "            src=\"kyoki.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7efc689f2eb0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "got_net = kyoki_word_network(df)\n",
    "got_net.show(\"kyoki.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e699c6fd-8f4e-4c57-af16-a7a32dd275af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'apple'), (1, 'banana'), (2, 'cherry')]\n"
     ]
    }
   ],
   "source": [
    "# サンプルのリスト\n",
    "my_list = [\"apple\", \"banana\", \"cherry\"]\n",
    "\n",
    "# 各要素とそのインデックスを組み合わせて新しいリストに格納する\n",
    "processed_list = [(index, item) for index, item in enumerate(my_list)]\n",
    "\n",
    "# 処理結果のリストを出力する\n",
    "print(processed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6652d-ab16-4e2e-b205-2e9a7772171b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
