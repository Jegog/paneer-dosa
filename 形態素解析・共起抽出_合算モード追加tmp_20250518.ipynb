{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87fd2589-1d5e-4f14-96d8-006355fb2fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import html\n",
    "import re\n",
    "import pandas as pd\n",
    "import logging\n",
    "import MeCab\n",
    "import fitz\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('/work_dir') \n",
    "import talknize_module_20240909 as tk\n",
    "import csv\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import datetime\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import girvan_newman\n",
    "import network_plot_module as npm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48455d40-f9ae-43dd-8ce6-d6c5c45c5f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改訂履歴\n",
    "# 2025/5/19　アウトプットする単語・共起・中心性指標のデータフレームにランクを付与\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0678da15-0802-46ec-b86e-2448c6ae9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#形態素解析の前処理\n",
    "#分析対象データの特性に対応するため、前処理はモジュールではなくコードとして記載\n",
    "#talknize_module.pyにも標準的な処理も記載し、適宜使い分け可能にする\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "#テキストファイルの各行に記載された文字列を、処理用文字列として整形・リスト化\n",
    "def text_to_list(file_path):\n",
    "\n",
    "    # 空のリストを作成\n",
    "    return_list = []\n",
    "    try:\n",
    "        # 指定されたファイルを読み込み、各行をリストに追加\n",
    "        with open(file_path, 'r') as file:\n",
    "            # ファイル内の各行をループし、行末の改行や余分な空白を除去してリストに格納\n",
    "            return_list = [line.strip() for line in file]\n",
    "    # ファイルが存在しない場合は例外を無視する\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    # リストを返す\n",
    "    return return_list\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "#形態素解析前のテキストデータ処理（\n",
    "#形態素解析の前に、無駄な記号やヘッダ・フッタ等の文言をテキストから除外\n",
    "def pre_tk(text, excl_list):\n",
    "\n",
    "    replaced_text = text\n",
    "\n",
    "    #exclusion_list処理前に処理する必要のあるもの\n",
    "    #【特例処理】除外処理前に、文頭のこれら記号は「箇条書き」とみなし、続く文言を一文として扱う\n",
    "    replaced_text = re.sub(r'^[■□▪▫▲△▶▷▸▹▼▽◆◇●〇]', '。\\n', replaced_text)\n",
    "    #replaced_text = re.sub(r'[〇●◇◆□■▶△▲▽▼▫▪▹▶▸]', '', replaced_text)#上記以外は除去\n",
    "\n",
    "    exclusion_list = []    \n",
    "    exclusion_file1 = \"userdic/exclusion_phrases1.txt\"  # 各企業の除外フレーズを記載したリスト\n",
    "    exclusion_file2 = \"userdic/exclusion_codes.txt\"  # その他記号・年月日・URL等を除外するためのリスト\n",
    "    #exclusion_file3 = \"userdic/exclusion_phrases2.txt\"  # 各企業の除外フレーズを記載したリスト2（pageinfoから都度取り込み）\n",
    "#    exclusion_list = text_to_list(exclusion_file1) + text_to_list(exclusion_file2)+ text_to_list(exclusion_file3)\n",
    "    exclusion_list = excl_list + text_to_list(exclusion_file1) + text_to_list(exclusion_file2)\n",
    "\n",
    "    for pattern in exclusion_list:\n",
    "        replaced_text = re.sub(pattern, ' ', replaced_text)\n",
    "\n",
    "    return replaced_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6beef27e-0a94-4441-976f-cc350cee1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#形態素解析の後処理\n",
    "#形態素解析結果（tokenリスト）から、ストップワード、特定の条件の文字列等を除外\n",
    "#import fitz\n",
    "#import re\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "\n",
    "def post_tk(token_list):\n",
    "    \n",
    "    replaced_list = token_list    \n",
    "\n",
    "    # stopwords（ファイルに格納）を除去\n",
    "    path_stopwords = \"userdic/stopwords.txt\"\n",
    "    stopwords = text_to_list(path_stopwords)\n",
    "    stopwords = list(OrderedDict.fromkeys(stopwords)) # 元の順序を保持しつつ、重複を除去（# Python 3.7以降）\n",
    "    replaced_list = [t for t in replaced_list if t not in stopwords]\n",
    "    \n",
    "    # ひらがなのみの要素を除去\n",
    "    kana_re = re.compile(\"^[ぁ-ゖ]+$\")\n",
    "    replaced_list = [t for t in replaced_list if not kana_re.match(t)]\n",
    "\n",
    "    # アルファベット1文字のみの要素を除去\n",
    "    alphabet_re = re.compile(\"^[a-zA-Z]$\")\n",
    "    replaced_list = [t for t in replaced_list if not alphabet_re.match(t)]\n",
    "\n",
    "    #特定の形態の数値要素を除去\n",
    "    number_re = re.compile(\"^[\\d,]+\")\n",
    "    replaced_list = [t for t in replaced_list if not number_re.match(t)]\n",
    "\n",
    "    \n",
    "    return replaced_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ac88d0-de44-4286-b76b-fb761ec05ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XMLからのテキスト抽出に用いる関数\n",
    "\n",
    "# ターゲットを判別する列に1が立っている行のkeyword行、すなわち読み込みたい目次名＝XMLタグを取得し、重複を除外\n",
    "def load_target_table_of_contents(df, target_column):\n",
    "    try:\n",
    "        return (\n",
    "            df.loc[df[target_column] == 1, 'keyword']\n",
    "            .dropna()\n",
    "            .drop_duplicates()\n",
    "            .apply(str.strip) #目次に余計なスペースがある時はこの行をコメントアウト\n",
    "            .tolist()\n",
    "        )\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_content_by_keywords(xml_path, keyword_list):\n",
    "    if not keyword_list:  # keyword_list が空なら空文字列を返す\n",
    "        return \"\"\n",
    "\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    collected_text = []\n",
    "\n",
    "    for page in root.findall('page'):\n",
    "        tag_elem = page.find('tag')\n",
    "        content_elem = page.find('content')\n",
    "\n",
    "        if tag_elem is not None and content_elem is not None:\n",
    "            tag_text = html.unescape((tag_elem.text or \"\").strip())\n",
    "\n",
    "            # タグをカンマや全角カンマで分割し、前後の空白も削除\n",
    "            tag_list = [t.strip() for t in re.split(r'[,]', tag_text) if t.strip()]\n",
    "\n",
    "            # いずれかのタグがキーワードに完全一致するか\n",
    "            if any(tag in keyword_list for tag in tag_list):\n",
    "                raw_text = content_elem.text\n",
    "                if raw_text:\n",
    "                    decoded_text = html.unescape(raw_text)\n",
    "                    cdata_match = re.search(r'<!\\[CDATA\\[(.*?)\\]\\]>', decoded_text, re.DOTALL)\n",
    "                    if cdata_match:\n",
    "                        collected_text.append(cdata_match.group(1).strip())\n",
    "\n",
    "    return '\\n\\n'.join(collected_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1afe2df7-5ca5-4bcb-9a5f-4497905a7e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ログファイルの設定\n",
    "log_filename = \"data/logfile_TextMining.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # ログレベルの設定 (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # ログのフォーマット\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename, mode=\"a\", encoding=\"utf-8\"),  # ファイルに記録\n",
    "        #logging.StreamHandler()  # コンソールにも出力\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb74e618-8b0b-4bb8-b2fe-eafa3f20d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#共通\n",
    "data_folder = 'data'\n",
    "base_folder = 'ir/2025'\n",
    "info_folder = 'info'\n",
    "target_info_file = 'target_company_20250524b.csv'\n",
    "target_info_path = data_folder + '/' + target_info_file\n",
    "eliminate_list=[]\n",
    "\n",
    "#==========================================================================\n",
    "#動作モードの指定\n",
    "#==========================================================================\n",
    "# ◆対象企業の読込方法   \n",
    "# 0はcsvファイルから順次読込・処理、\n",
    "# 1は個別に指定\n",
    "company_selection_mode = 0\n",
    "\n",
    "# ◆XMLからの抽出範囲   \n",
    "# 0はcontents.csvで指定した範囲指定全て、\n",
    "# 1はマニュアル指定（リスト）\n",
    "column_selection_mode = 1\n",
    "\n",
    "# column_selection_mode = 1の場合に有効\n",
    "# 0は指定した指定した範囲ごとに計算、\n",
    "# 1はリストで指定した範囲を合算して計算\n",
    "column_calculation_mode = 1\n",
    "\n",
    "# ◆出力ファイル出力方法　\n",
    "# 0は一つの企業ごとに複数の指定範囲を１ファイルで出力、\n",
    "# 1は企業ごと・指定範囲ごとに出力\n",
    "# ファイル名：　0:xxx_combined 1:xxx_指定範囲名\n",
    "file_output_mode = 1\n",
    "\n",
    "# ==========================================================================\n",
    "# 動作モードに応じた設定（csvから読み込まない場合、以下に記述）\n",
    "# ==========================================================================\n",
    "# company_selection_mode = 1 の場合の個別指定項目\n",
    "security_code = '60130'\n",
    "company_name = 'タクマ'\n",
    "industry_sector = '機械'\n",
    "target_folder = '60130_タクマ'\n",
    "pdf_filename =  'report-2024A3.pdf'\n",
    "temp_list = [security_code, company_name, industry_sector, target_folder, pdf_filename]\n",
    "\n",
    "# column_selection_mode = 1 の場合の抽出条件グループ\n",
    "# ここで定義したものを会社ごとに参照\n",
    "# def_target_columns = ['経営者', '価値創造', 'ESG', '財務', '知的資本', '人的資本, '社会関係資本', '流通', 'マーケティング', 'all']\n",
    "def_target_columns = ['経営者', '価値創造', '知的資本']\n",
    "# def_target_columns = ['経営者', '価値創造', 'ESG', '財務', '知的資本', '人的資本', '社会関係資本','all']\n",
    "# contents.csvのうち抽出条件範囲でない列を除外：これはmodeによらず共通\n",
    "excluded_columns = ['keyword', 'page', 'origin', '表示']\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# 読込対象企業の企業名、格納フォルダ等の情報\n",
    "# company_selection_modeに応じた処理\n",
    "# company_selection_mode == 0の場合は指定したファイル（csv)から\n",
    "if company_selection_mode == 0 :\n",
    "    df_target_company_info = pd.read_csv(target_info_path)\n",
    "else:\n",
    "    df_target_company_info = pd.DataFrame([temp_list],columns=['コード','企業名','業種分類','格納フォルダ','統合報告書PDF'])\n",
    "#df_target_company_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea1954e5-2c7f-4343-8291-f2d94b0a1cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ir/2025/45300_久光製薬/Integrated_report2024.xml\n",
      "ir/2025/45300_久光製薬/Integrated_report2024.xml processed at 20250524-142526.\n",
      "Processing ir/2025/60130_タクマ/report-2024A3.xml\n",
      "ir/2025/60130_タクマ/report-2024A3.xml processed at 20250524-142538.\n",
      "All process done.\n"
     ]
    }
   ],
   "source": [
    "# 以下、企業単位で処理・出力\n",
    "for index, row in df_target_company_info.iterrows():\n",
    "    # target_folder = security_code + '_' + company_name\n",
    "\n",
    "    df_count_combined = pd.DataFrame()\n",
    "    df_combination_ex_combined = pd.DataFrame()    \n",
    "    df_centrality_combined = pd.DataFrame()    \n",
    "\n",
    "    # ファイル識別に利用する年月日文字列の取得（企業の処理単位で取得）\n",
    "    # 現在の時刻情報を取得\n",
    "    now = datetime.datetime.now()\n",
    "    # 年月日と時刻の文字列を生成\n",
    "    date_time_string = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    time_string = now.strftime(\"%H%M%S\")\n",
    "    \n",
    "    # 行ごとの値を読み込み\n",
    "    row_security_code = row['コード']\n",
    "    row_company_name = row['企業名']\n",
    "    row_industy_sector = row['業種分類']\n",
    "    row_pdf_filename =  row['統合報告書PDF']\n",
    "    row_xml_filename = row_pdf_filename[:len(row_pdf_filename)-4] + '.xml'\n",
    "    row_target_folder = base_folder + '/' +row['格納フォルダ']#PDFと作成したXMLの格納先となるフォルダ\n",
    "    row_xml_path = row_target_folder + '/' + row_xml_filename\n",
    "    row_info_folder = row_target_folder + '/' + info_folder\n",
    "    row_contents_csv = row_info_folder + '/' + 'contents.csv'\n",
    "    # print(row_security_code, row_info_folder, row_xml_filename)\n",
    "    print(f\"Processing {row_xml_path}\")\n",
    "\n",
    "    # contents.CSVを読み込む\n",
    "    df_tbl_of_cnt = pd.read_csv(row_contents_csv)\n",
    "    # print(df_tbl_of_cnt)\n",
    "\n",
    "    # column_selection_mode == 0の場合、テキストデータを取得する範囲の名称を動的に抽出\n",
    "    #  def_target_columnsはモード設定のフラグ直後に定義されているので、それを条件に応じ書き換え\n",
    "    if column_selection_mode == 0:\n",
    "        if df_tbl_of_cnt.empty:\n",
    "            target_columns = []\n",
    "        else:\n",
    "            #csvファイルからカラム名を取得して設定\n",
    "            target_columns = [col for col in df_tbl_of_cnt.columns if col not in excluded_columns]\n",
    "\n",
    "    # column_selection_mode == 1の場合は、モード設定のフラグ直後に記載したtarget_columnsを参照\n",
    "    # ただし、手動で記載したものを読み込み\n",
    "    else:\n",
    "    # column_selection_mode == 1 の場合、\n",
    "        # column_caluculation_mode == 0なら、def_target_columnsをそのまま参照\n",
    "        if column_calculation_mode ==0:\n",
    "            target_columns = def_target_columns\n",
    "\n",
    "        # column_caluculation_mode == 1の場合は、def_target_columnsのいずれかでフラグが立った目次項目を使用、target_columnsもdef_target_columnsを加工して設定\n",
    "        if column_calculation_mode == 1:  \n",
    "            # 読み込む目次項目（target_table_of_contents）を設定：target_columns内の項目のいずれかでフラグが立った目次項目を対象\n",
    "            merged_target_table_of_contents = df_tbl_of_cnt[df_tbl_of_cnt[def_target_columns].eq(1).any(axis=1)]['keyword'].dropna().unique().tolist()\n",
    "            # print(merged_target_table_of_contents)\n",
    "            # 合算モードの場合は、def_target_columnsを記載されたリスト項目によって書き換え\n",
    "            # def_target_columns = ['AAA','BBB']　なら、target_columns = ['AAA+BBB']にしてしまう\n",
    "            target_columns = ['+'.join(def_target_columns)]\n",
    "\n",
    "    # print(f\"selection = {column_selection_mode} calculation = {column_calculation_mode}\")\n",
    "    # print(f\"target_columns = {target_columns}\")\n",
    "    # print(target_columns)\n",
    "\n",
    "    # 各ターゲット列（target_columnsリストで指定された列）に応じた処理\n",
    "    if target_columns == []:\n",
    "        logging.info(\"target_columnsが空です。ターゲット列の処理はスキップします。\")\n",
    "    else:\n",
    "        for col in target_columns:\n",
    "            # print(f\"col = {col}\")            \n",
    "            # print(f\"\\n=== タグ: {col} ===\\n\")\n",
    "            if column_selection_mode == 0:\n",
    "                target_table_of_contents = load_target_table_of_contents(df_tbl_of_cnt, col)\n",
    "            else:\n",
    "                if column_calculation_mode == 0:  \n",
    "                    target_table_of_contents = load_target_table_of_contents(df_tbl_of_cnt, col)\n",
    "                else:\n",
    "                    target_table_of_contents = merged_target_table_of_contents\n",
    "\n",
    "            # print(f\"calc_mode={column_calculation_mode}\")\n",
    "            # print(f\"target_table_of_contents = {target_table_of_contents}\")\n",
    "            output_text = extract_content_by_keywords(row_xml_path, target_table_of_contents)\n",
    "            # print(output_text)\n",
    "    \n",
    "            # '。'で改行、一文の範囲を明確にする\n",
    "            separated_text = re.sub(r'。','。\\n', output_text)\n",
    "            tokenized_text_list = post_tk(tk.mecab_tokenizer(pre_tk(separated_text, eliminate_list)))\n",
    "            # Merged_Extracted_tokenized_list = [item for _, sublist in Extracted_tokenized_page_text for item in sublist]\n",
    "            # print(tokenized_text_list)\n",
    "    \n",
    "            #辞書形式で単語をカウント\n",
    "            counter = Counter(tokenized_text_list)\n",
    "            # 単語、件数をDataFrameに格納\n",
    "            df_count = pd.DataFrame(list(counter.items()), columns=['単語', '件数'])\n",
    "            # DataFrameを件数でソート\n",
    "            df_count = df_count.sort_values(by='件数', ascending=False)\n",
    "\n",
    "            # df_countが[]の場合（tokenized_listが[]の場合）は、単語リスト出力とワードクラウドの作成をスキップ\n",
    "            if df_count.empty:\n",
    "                logging.info(f\"{row_company_name}で、{col}で指定した範囲で抽出された単語がありません。\")\n",
    "            else:\n",
    "                # 新しい列を追加\n",
    "                df_count.insert(0, '業種', row_industy_sector)\n",
    "                df_count.insert(1, 'コード', row_security_code)\n",
    "                df_count.insert(2, '企業名', row_company_name)\n",
    "                df_count.insert(3, '分析範囲', col)\n",
    "        \n",
    "                #print(df_count)\n",
    "\n",
    "                # 単語の件数にランクを付与してdfに格納\n",
    "                # 対象のスコア列\n",
    "                score_columns = [\"件数\"]\n",
    "\n",
    "                # 分析範囲ごとに順位を付けて新しい列を追加（NaNも考慮）\n",
    "                for scol in score_columns:\n",
    "                    # 念のため、数値変換（非数値はNaN化される）\n",
    "                    df_count[scol] = pd.to_numeric(df_count[scol], errors='coerce')\n",
    "                \n",
    "                    # 順位列を追加（NaNは自動的に除外される）\n",
    "                    df_count[f\"{scol}_Rank\"] = df_count.groupby(\"分析範囲\")[scol].rank(method=\"min\", ascending=False)\n",
    "                    \n",
    "                    # 整数として表示する場合、NaN以外をint化（見やすさのため）\n",
    "                    df_count[f\"{scol}_Rank\"] = df_count[f\"{scol}_Rank\"].apply(lambda x: int(x) if pd.notnull(x) else None)\n",
    "\n",
    "                \n",
    "                # 結果をCSVファイルに出力\n",
    "                #個別に出力するパターン\n",
    "                if file_output_mode == 1: \n",
    "                    file_name = f\"output/WordList/{row_security_code}_{row_company_name}_{col}_Word_list_{date_time_string}.csv\"\n",
    "                    df_count.to_csv(file_name, encoding=\"utf_8_sig\", index=False)        \n",
    "                else:\n",
    "                    df_count_combined = pd.concat([df_count_combined, df_count], ignore_index=True)            \n",
    "    \n",
    "                #抽出単語によるワードクラウド作成\n",
    "                #ワードクラウド用設定\n",
    "                # 日本語フォントのパスを指定\n",
    "                jp_font_path = '/usr/share/fonts/opentype/ipaexfont-gothic/ipaexg.ttf'\n",
    "                # ワードクラウドのフォーマット指定\n",
    "                wordcloud = WordCloud(width=800, height=400, background_color='white',font_path=jp_font_path)\n",
    "                # 単語とその頻度を辞書形式に変換\n",
    "                word_freq = {word: freq for word, freq in zip(df_count['単語'], df_count['件数'])}\n",
    "                # ワードクラウドの生成\n",
    "                wordcloud.generate_from_frequencies(word_freq)\n",
    "                # プロット\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.imshow(wordcloud, interpolation='bilinear')\n",
    "                plt.axis('off')\n",
    "                #plt.show()\n",
    "                \n",
    "                # 結果をpngファイルに出力\n",
    "                file_name_wordcloud = f'output/WordCloud/{row_security_code}_{row_company_name}_{col}_WordCloud_{date_time_string}.png'\n",
    "                wordcloud.to_file(file_name_wordcloud)\n",
    "                plt.close()\n",
    "                #plt.clf()\n",
    "                #plt.close()\n",
    "                #tf-idf用に、ファイル名、企業名、トークンを出力\n",
    "                #とりあえず略\n",
    "    \n",
    "                #各文中の、形態素組み合わせを作る\n",
    "                #一文単位でトークンリストを作る\n",
    "                #print(output_text)\n",
    "                #print(f\"--------------------\")\n",
    "                #temp_sentences = [sentence for sentence in re.split(\"。\", pre_tk(output_text, eliminate_list))]\n",
    "                #print(temp_sentences)\n",
    "                #print(f\"--------------------\")\n",
    "\n",
    "                #sentences = [post_tk(tk.mecab_tokenizer(sentence)) for sentence in re.split(\"。\", output_text)]\n",
    "                #現在の改行か、句点「。」で文章を区切り、要素リストに区分する\n",
    "                sentences = [post_tk(tk.mecab_tokenizer(sentence)) for sentence in re.split(r'[。\\n]', pre_tk(output_text, eliminate_list))]\n",
    "                #空の要素リスト（空行か、テキストマイニングの結果リスト要素がなくなったもの）を除外\n",
    "                sentences = [lst for lst in sentences if lst]\n",
    "                #print(sentences)\n",
    "                #input()\n",
    "\n",
    "                #各文のトークンの組み合わせを作る\n",
    "                #従来は全部の組み合わせ\n",
    "                #sentence_combs = [list(itertools.combinations(sentence,2)) for sentence in sentences]\n",
    "                #隣接する指定文字数内のトークンの組み合わせに限定\n",
    "                max_distance = 15\n",
    "                sentence_combs = [\n",
    "                    [(sentence[i], sentence[j]) \n",
    "                     for i in range(len(sentence)) \n",
    "                     for j in range(i + 1, min(i + 1 + max_distance, len(sentence)))]\n",
    "                    for sentence in sentences\n",
    "                ]\n",
    "\n",
    "                #組み合わせた2つの形態素の並びをソート\n",
    "                words_combs = [[tuple(sorted(words)) for words in sentence] for sentence in sentence_combs]\n",
    "                #print(words_combs[0][:30])\n",
    "                target_combs = []\n",
    "                for words_comb in words_combs:\n",
    "                    target_combs.extend(words_comb)\n",
    "                ct = Counter(target_combs)\n",
    "                #print(ct)\n",
    "                df_combination = pd.DataFrame([{\"1番目\" : i[0][0], \"2番目\": i[0][1], \"count\":i[1]} for i in ct.most_common()])\n",
    "                df_combination_ex = df_combination.copy()\n",
    "                # 新しい列を追加\n",
    "                df_combination_ex.insert(0, '業種', row_industy_sector)\n",
    "                df_combination_ex.insert(1, 'コード', row_security_code)\n",
    "                df_combination_ex.insert(2, '企業名', row_company_name)\n",
    "                df_combination_ex.insert(3, '分析範囲', col)\n",
    "        \n",
    "                #print(df_combination_ex)\n",
    "                # 単語の件数にランクを付与してdfに格納\n",
    "                # 対象のスコア列\n",
    "                score_columns = [\"count\"]\n",
    "\n",
    "                # 分析範囲ごとに順位を付けて新しい列を追加（NaNも考慮）\n",
    "                for scol in score_columns:\n",
    "                    # 念のため、数値変換（非数値はNaN化される）\n",
    "                    df_combination_ex[scol] = pd.to_numeric(df_combination_ex[scol], errors='coerce')\n",
    "                \n",
    "                    # 順位列を追加（NaNは自動的に除外される）\n",
    "                    df_combination_ex[f\"{scol}_Rank\"] = df_combination_ex.groupby(\"分析範囲\")[scol].rank(method=\"min\", ascending=False)\n",
    "                    \n",
    "                    # 整数として表示する場合、NaN以外をint化（見やすさのため）\n",
    "                    df_combination_ex[f\"{scol}_Rank\"] = df_combination_ex[f\"{scol}_Rank\"].apply(lambda x: int(x) if pd.notnull(x) else None)\n",
    "        \n",
    "                #ファイル出力\n",
    "                if file_output_mode == 1: \n",
    "                    file_name_comb = f\"output/Co-Occurrence/{row_security_code}_{row_company_name}_{col}_Co-Occurrence_{date_time_string}.csv\"\n",
    "                    df_combination_ex.to_csv(file_name_comb, encoding=\"utf_8_sig\", index=False)\n",
    "                else:\n",
    "                    df_combination_ex_combined = pd.concat([df_combination_ex_combined, df_combination_ex], ignore_index=True)  \n",
    "        \n",
    "                #########################################################\n",
    "                # 分析対象とする共起単語の組み合わせ数（ノード数）を指定\n",
    "                analyzed_links = 500\n",
    "                limited_df = df_combination.head(analyzed_links)\n",
    "                #########################################################\n",
    "        \n",
    "                # DataFrameからネットワークを作成\n",
    "                G = nx.from_pandas_edgelist(limited_df, '1番目', '2番目', ['count'])\n",
    "        \n",
    "                # 各ノードの中心性を計算\n",
    "                try:\n",
    "                    degree_centrality = nx.degree_centrality(G)\n",
    "                except:\n",
    "                    degree_centrality = {node: '' for node in G.nodes()}\n",
    "                    \n",
    "                try:\n",
    "                    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "                except:\n",
    "                    betweenness_centrality = {node: '' for node in G.nodes()}\n",
    "                \n",
    "                try:\n",
    "                    closeness_centrality = nx.closeness_centrality(G)\n",
    "                except:\n",
    "                    closeness_centrality = {node: '' for node in G.nodes()}\n",
    "                \n",
    "                try:\n",
    "                    eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "                except:\n",
    "                    eigenvector_centrality = {node: '' for node in G.nodes()}\n",
    "                \n",
    "                try:\n",
    "                    katz_centrality = nx.katz_centrality(G)\n",
    "                except:\n",
    "                    katz_centrality = {node: '' for node in G.nodes()}\n",
    "                \n",
    "                # Girvan-Newmanアルゴリズムでコミュニティに分割\n",
    "                comp = girvan_newman(G)\n",
    "                communities = tuple(sorted(c) for c in next(comp))\n",
    "                \n",
    "                # 各ノードがどのコミュニティに属するかを記録\n",
    "                community_map = {}\n",
    "                for i, community in enumerate(communities):\n",
    "                    for node in community:\n",
    "                        community_map[node] = i\n",
    "                \n",
    "                # 中心性を新しいデータフレームに格納\n",
    "                df_centrality = pd.DataFrame({\n",
    "                    'コード': row_security_code,\n",
    "                    '企業名': row_company_name,\n",
    "                    '分析範囲': col,\n",
    "                    'Node': list(G.nodes()),\n",
    "                    'Degree Centrality': [degree_centrality[node] for node in G.nodes()],\n",
    "                    'Betweenness Centrality': [betweenness_centrality[node] for node in G.nodes()],\n",
    "                    'Closeness Centrality': [closeness_centrality[node] for node in G.nodes()],\n",
    "                    'Eigenvector Centrality': [eigenvector_centrality[node] for node in G.nodes()],\n",
    "                #    'Katz Centrality': [katz_centrality[node] for node in G.nodes()],\n",
    "                    'Community': [community_map[node] for node in G.nodes()]  # コミュニティ情報を追加\n",
    "                    })\n",
    "                #print(df_centrality)\n",
    "\n",
    "                # 対象のスコア列\n",
    "                score_columns = [\n",
    "                    \"Degree Centrality\",\n",
    "                    \"Betweenness Centrality\",\n",
    "                    \"Closeness Centrality\",\n",
    "                    \"Eigenvector Centrality\"\n",
    "                ]\n",
    "\n",
    "                # 分析範囲ごとに順位を付けて新しい列を追加（NaNも考慮）\n",
    "                for scol in score_columns:\n",
    "                    # 念のため、数値変換（非数値はNaN化される）\n",
    "                    df_centrality[scol] = pd.to_numeric(df_centrality[scol], errors='coerce')\n",
    "                \n",
    "                    # 順位列を追加（NaNは自動的に除外される）\n",
    "                    df_centrality[f\"{scol}_Rank\"] = df_centrality.groupby(\"分析範囲\")[scol].rank(method=\"min\", ascending=False)\n",
    "                    \n",
    "                    # 整数として表示する場合、NaN以外をint化（見やすさのため）\n",
    "                    df_centrality[f\"{scol}_Rank\"] = df_centrality[f\"{scol}_Rank\"].apply(lambda x: int(x) if pd.notnull(x) else None)\n",
    "\n",
    "\n",
    "                if file_output_mode == 1: \n",
    "                    file_name_centrality = f\"output/Centrality/{row_security_code}_{row_company_name}_{col}_Centrality_{analyzed_links}_{date_time_string}.csv\"\n",
    "                    df_centrality.to_csv(file_name_centrality, encoding=\"utf_8_sig\", index=False)\n",
    "                else:\n",
    "                    df_centrality_combined = pd.concat([df_centrality_combined, df_centrality], ignore_index=True)\n",
    "                \n",
    "                #ネットワーク図を描画、ファイル出力\n",
    "                got_net = npm.kyoki_word_network(limited_df)\n",
    "                #フィルタボタンを表示させる場合は、set_optionを無効にする必要あり\n",
    "                #got_net.show_buttons(filter_=['physics'])\n",
    "                got_net.set_options(\"\"\"\n",
    "                const options = {\n",
    "                  \"physics\": {\n",
    "                    \"forceAtlas2Based\": {\n",
    "                      \"centralGravity\": 0.1,\n",
    "                      \"springLength\": 25,\n",
    "                      \"springConstant\": 0.1\n",
    "                    },\n",
    "                    \"minVelocity\": 0.75,\n",
    "                    \"solver\": \"forceAtlas2Based\"\n",
    "                  }\n",
    "                }\n",
    "                \"\"\")\n",
    "                file_name_kyoki = f'output/Kyoki/{row_security_code}_{row_company_name}_{col}_kyoki_{analyzed_links}_{date_time_string}_.html'\n",
    "                got_net.show(file_name_kyoki)\n",
    "\n",
    "    \n",
    "    if file_output_mode != 1:\n",
    "        file_name = f\"output/WordList/{row_security_code}_{row_company_name}_combined_Word_list_{date_time_string}.csv\"\n",
    "        df_count_combined.to_csv(file_name, encoding=\"utf_8_sig\", index=False)\n",
    "\n",
    "        file_name_comb = f\"output/Co-Occurrence/{row_security_code}_{row_company_name}_combined_Co-Occurrence_{date_time_string}.csv\"\n",
    "        df_combination_ex_combined.to_csv(file_name_comb, encoding=\"utf_8_sig\", index=False)\n",
    "\n",
    "        file_name_centrality = f\"output/Centrality/{row_security_code}_{row_company_name}_combined_Centrality_{analyzed_links}_{date_time_string}.csv\"\n",
    "        df_centrality_combined.to_csv(file_name_centrality, encoding=\"utf_8_sig\", index=False)\n",
    "\n",
    "    print(f\"{row_target_folder}/{row_xml_filename} processed at {date_time_string}.\")\n",
    "    logging.info(f\"{row_target_folder}/{row_xml_filename} processed at {date_time_string}.\")\n",
    "\n",
    "print(f\"All process done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73665f77-a123-4770-ac1a-5c0dc4da54f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4430ba-bf45-4fe8-9f97-f8a57c20689e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
