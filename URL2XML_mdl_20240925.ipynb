{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da2798d5-752b-4b3c-98fc-bf0d34b69956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特定のdivデータがXMLファイルに作成されました。\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 取得したいURLを指定\n",
    "url = \"https://www.kirinholdings.com/jp/domains/health_science/kyowahakko-bio/\"\n",
    "\n",
    "\n",
    "# URLからHTMLを取得\n",
    "response = requests.get(url)\n",
    "\n",
    "# レスポンスが成功したか確認\n",
    "if response.status_code == 200:\n",
    "    # バイトデータ (response.content) を使い、from_encoding を指定\n",
    "    soup = BeautifulSoup(response.content, 'lxml', from_encoding='utf-8')\n",
    "\n",
    "    #print(soup)\n",
    "    # 特定のdivを取得\n",
    "#    div_content = soup.find(\"div\", {\"class\": \"content-fixed\", \"data-fixed\": \"\"})\n",
    "#    div_content = soup.find(\"div\", {\"data-fixed\": True})\n",
    "    div_content = soup.find(\"main\", {\"id\": 'l-content'})\n",
    "    \n",
    "    if div_content:\n",
    "        # div内のデータをXML形式で出力\n",
    "        xml_content = div_content.prettify()\n",
    "\n",
    "        # XMLをファイルに保存 (バイトデータとして保存)\n",
    "        with open(\"output.xml\", \"wb\") as file:\n",
    "            file.write(xml_content.encode('utf-8'))\n",
    "\n",
    "        print(\"特定のdivデータがXMLファイルに作成されました。\")\n",
    "    else:\n",
    "        print(\"指定されたdivが見つかりませんでした。\")\n",
    "else:\n",
    "    print(f\"URLにアクセスできません。ステータスコード: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f43b463-889a-422f-b9f4-1a3b9197a058",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# すべての<div class=\"m-txt-ttl\">をXML形式に変換\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m div \u001b[38;5;129;01min\u001b[39;00m div_m_txt_ttl:\n\u001b[0;32m---> 27\u001b[0m     xml_content \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mdiv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprettify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#  <div data-fixed=\"\">をXML形式に変換\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m div_data_fixed:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/bs4/element.py:1926\u001b[0m, in \u001b[0;36mTag.prettify\u001b[0;34m(self, encoding, formatter)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Pretty-print this PageElement as a string.\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \n\u001b[1;32m   1918\u001b[0m \u001b[38;5;124;03m:param encoding: The eventual encoding of the string. If this is None,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;124;03m    (otherwise).\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1928\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(encoding, \u001b[38;5;28;01mTrue\u001b[39;00m, formatter\u001b[38;5;241m=\u001b[39mformatter)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/bs4/element.py:1698\u001b[0m, in \u001b[0;36mTag.decode\u001b[0;34m(self, indent_level, eventual_encoding, formatter, iterator)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# First off, turn a non-Formatter `formatter` into a Formatter\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;66;03m# object. This will stop the lookup from happening over and\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;66;03m# over again.\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatter, Formatter):\n\u001b[0;32m-> 1698\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformatter_for_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indent_level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1701\u001b[0m     indent_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/bs4/element.py:228\u001b[0m, in \u001b[0;36mPageElement.formatter_for_name\u001b[0;34m(self, formatter)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatter, Callable):\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m c(entity_substitution\u001b[38;5;241m=\u001b[39mformatter)\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREGISTRY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'xml'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 取得したいURLを指定\n",
    "url = \"https://www.kirinholdings.com/jp/domains/health_science/kyowahakko-bio/\"\n",
    "\n",
    "# URLからHTMLを取得\n",
    "response = requests.get(url)\n",
    "\n",
    "# レスポンスが成功したか確認\n",
    "if response.status_code == 200:\n",
    "    # バイトデータ (response.content) を使い、from_encoding を指定\n",
    "    soup = BeautifulSoup(response.content, 'lxml', from_encoding='utf-8')\n",
    "\n",
    "    # <div class=\"m-txt-ttl\">タグを取得\n",
    "    div_m_txt_ttl = soup.find_all('div', {'class': 'm-txt-ttl'})\n",
    "    \n",
    "    # <div data-fixed=\"\">タグを取得\n",
    "    div_data_fixed = soup.find('div', {'data-fixed': True})\n",
    "    \n",
    "    # 5. 取得したタグをXML形式に変換して出力\n",
    "    xml_content = \"\"\n",
    "    \n",
    "    # すべての<div class=\"m-txt-ttl\">をXML形式に変換\n",
    "    for div in div_m_txt_ttl:\n",
    "        xml_content += div.prettify(formatter=\"xml\") + \"\\n\"\n",
    "    \n",
    "    #  <div data-fixed=\"\">をXML形式に変換\n",
    "    if div_data_fixed:\n",
    "        xml_content += div_data_fixed.prettify(formatter=\"xml\") + \"\\n\"\n",
    "        # XMLをファイルに保存 (バイトデータとして保存)\n",
    "        with open(\"output.xml\", \"wb\") as file:\n",
    "            file.write(xml_content.encode('utf-8'))\n",
    "        print(\"特定のdivデータをXMLファイルに格納しました。\")\n",
    "    else:\n",
    "        print(\"指定されたdivが見つかりませんでした。\")\n",
    "else:\n",
    "    print(f\"URLにアクセスできません。ステータスコード: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb47624-5f27-49c1-a9fb-211d3bd4353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div data-fixed=\"\">\n",
      " <p class=\"m-txt-p\">\n",
      "  2024年5月31日\n",
      " </p>\n",
      " <p class=\"m-txt-copy hide-sp\" id=\"headline-1712797041\">\n",
      "  アミノ酸事業のあらゆる選択肢を念頭に置いた構造改革の検討と、\n",
      "  <br/>\n",
      "  スペシャリティ素材への注力による収益化を実現する\n",
      "  <br/>\n",
      " </p>\n",
      " <p class=\"m-txt-copy hide-pc hide-tab\" id=\"headline-1713241603\">\n",
      "  アミノ酸事業のあらゆる選択肢を念頭に置いた構造改革の検討と、スペシャリティ素材への注力による収益化を実現する\n",
      " </p>\n",
      " <div class=\"m-list-image position-normal hide-sp\">\n",
      "  <ul data-gs=\"12\" data-gs-sp=\"1\">\n",
      "   <li data-gs-col=\"12\">\n",
      "    <figure class=\"item-inner\">\n",
      "     <p class=\"item-img\">\n",
      "      <img alt=\"協和発酵バイオ　2023年実績　売上収益　514億円　事業利益　-85億円 協和発酵バイオ　2024年計画　売上収益　589億円　事業利益　-30億円\" decoding=\"async\" src=\"/jp/domains/files/images/p_kyowahakko-bio_01_pc.jpg\"/>\n",
      "     </p>\n",
      "    </figure>\n",
      "   </li>\n",
      "  </ul>\n",
      " </div>\n",
      " <div class=\"m-list-image position-normal hide-pc hide-tab\">\n",
      "  <ul data-gs=\"12\" data-gs-sp=\"1\">\n",
      "   <li data-gs-col=\"12\">\n",
      "    <figure class=\"item-inner\">\n",
      "     <p class=\"item-img\">\n",
      "      <img alt=\"協和発酵バイオ　2023年実績　売上収益　514億円　事業利益　-85億円 協和発酵バイオ　2024年計画　売上収益　589億円　事業利益　-30億円\" decoding=\"async\" src=\"/jp/domains/files/images/p_kyowahakko-bio_01_sp.jpg\"/>\n",
      "     </p>\n",
      "    </figure>\n",
      "   </li>\n",
      "  </ul>\n",
      " </div>\n",
      " <h2 class=\"m-txt-ttl2\" id=\"headline-1712797207\">\n",
      "  2023年振り返り\n",
      " </h2>\n",
      " <ul class=\"m-list-desc\">\n",
      "  <li>\n",
      "   売上収益はアミノ酸事業の苦戦があるも、シチコリンが健康食品用を中心に販売が堅調に推移したことなどにより増収となりました。\n",
      "  </li>\n",
      "  <li>\n",
      "   一方で事業利益は、燃料費などの変動コストの増加や競争環境の激化、ワクチン製造原料特需の消失など、アミノ酸事業を中心に厳しい事業環境が継続しており、減益となりました。ヒトミルクオリゴ糖は、各国で規制当局からの承認手続きが進んでいます。\n",
      "  </li>\n",
      " </ul>\n",
      " <h2 class=\"m-txt-ttl2\" id=\"headline-1712797231\">\n",
      "  2024年計画\n",
      " </h2>\n",
      " <ul class=\"m-list-desc\">\n",
      "  <li>\n",
      "   アミノ酸事業を中心としたコスト削減や伸長するシチコリンへの注力により赤字幅の縮小を計画しています。シチコリンは特に収益性の高い健康食品用に注力し、ヒトミルクオリゴ糖は各国における早期の認証取得に引き続き取り組みます。また、各素材の用途拡大も図っていきます。\n",
      "  </li>\n",
      " </ul>\n",
      "</div>\n",
      "\n",
      "<div class=\"m-txt-ttl\">\n",
      " <div class=\"ttl-outer\">\n",
      "  <div class=\"ttl-inner\">\n",
      "   <h1 class=\"ttl-txt\">\n",
      "    協和発酵バイオ\n",
      "   </h1>\n",
      "  </div>\n",
      " </div>\n",
      "</div>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. URLからHTMLを取得\n",
    "url = \"https://www.kirinholdings.com/jp/domains/health_science/kyowahakko-bio/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# 2. BeautifulSoupでHTMLをlxmlパーサーで解析\n",
    "soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "# 3. 特定の<div data-fixed=\"\">タグを取得\n",
    "div_data_fixed = soup.find('div', {'data-fixed': True})\n",
    "\n",
    "# 4. <div class=\"m-txt-ttl\">タグを取得\n",
    "div_m_txt_ttl = soup.find_all('div', {'class': 'm-txt-ttl'})\n",
    "\n",
    "# 5. 取得したタグをXML形式に変換して出力\n",
    "xml_content = \"\"\n",
    "\n",
    "# まず、<div data-fixed=\"\">をXML形式に変換\n",
    "if div_data_fixed:\n",
    "    xml_content += div_data_fixed.prettify() + \"\\n\"\n",
    "else:\n",
    "    print(\"<div data-fixed=''>が見つかりませんでした。\")\n",
    "\n",
    "# 次に、すべての<div class=\"m-txt-ttl\">をXML形式に変換\n",
    "for div in div_m_txt_ttl:\n",
    "    xml_content += div.prettify() + \"\\n\"\n",
    "\n",
    "# 結果を表示\n",
    "print(xml_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f759b862-613b-46d9-9e60-28326b34d62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特定のdivデータをXMLファイルに格納しました。\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 取得したいURLを指定\n",
    "url = \"https://www.kirinholdings.com/jp/domains/health_science/kyowahakko-bio/\"\n",
    "\n",
    "# URLからHTMLを取得\n",
    "response = requests.get(url)\n",
    "\n",
    "# レスポンスが成功したか確認\n",
    "if response.status_code == 200:\n",
    "    # バイトデータ (response.content) を使い、from_encoding を指定\n",
    "    soup = BeautifulSoup(response.content, 'lxml', from_encoding='utf-8')\n",
    "\n",
    "    # <div class=\"m-txt-ttl\">タグを取得\n",
    "    div_m_txt_ttl = soup.find_all('div', {'class': 'm-txt-ttl'})\n",
    "    \n",
    "    # <div data-fixed=\"\">タグを取得\n",
    "    div_data_fixed = soup.find('div', {'data-fixed': True})\n",
    "    \n",
    "    # 取得したタグをXML形式に変換して出力 (ただし、formatter=\"xml\"は使わない)\n",
    "    xml_content = \"\"\n",
    "\n",
    "    # すべての<div class=\"m-txt-ttl\">を変換\n",
    "    for div in div_m_txt_ttl:\n",
    "        xml_content += div.prettify() + \"\\n\"\n",
    "    \n",
    "    # <div data-fixed=\"\">を変換\n",
    "    if div_data_fixed:\n",
    "        xml_content += div_data_fixed.prettify() + \"\\n\"\n",
    "\n",
    "        # XMLをファイルに保存 (バイトデータとして保存)\n",
    "        with open(\"output.xml\", \"wb\") as file:\n",
    "            file.write(xml_content.encode('utf-8'))\n",
    "        print(\"特定のdivデータをXMLファイルに格納しました。\")\n",
    "    else:\n",
    "        print(\"指定された<div data-fixed=''>が見つかりませんでした。\")\n",
    "else:\n",
    "    print(f\"URLにアクセスできません。ステータスコード: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d67d961b-f934-4028-a60f-22b6d63e9a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特定のdivデータをXMLファイルに格納しました。\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 取得したいURLを指定\n",
    "url = \"https://www.kirinholdings.com/jp/domains/health_science/kyowahakko-bio/\"\n",
    "\n",
    "# URLからHTMLを取得\n",
    "response = requests.get(url)\n",
    "\n",
    "# レスポンスが成功したか確認\n",
    "if response.status_code == 200:\n",
    "    # バイトデータ (response.content) を使い、from_encoding を指定\n",
    "    soup = BeautifulSoup(response.content, 'lxml', from_encoding='utf-8')\n",
    "\n",
    "    # <div class=\"m-txt-ttl\">タグを取得\n",
    "    div_m_txt_ttl = soup.find_all('div', {'class': 'm-txt-ttl'})\n",
    "    \n",
    "    # <div data-fixed=\"\">タグを取得\n",
    "    div_data_fixed = soup.find('div', {'data-fixed': True})\n",
    "    \n",
    "    # 取得したタグをXML形式に変換して出力 (ただし、formatter=\"xml\"は使わない)\n",
    "    xml_content = \"\"\n",
    "\n",
    "    # すべての<div class=\"m-txt-ttl\">を変換\n",
    "    for div in div_m_txt_ttl:\n",
    "        xml_content += div.prettify() + \"\\n\"\n",
    "    \n",
    "    # 特定のタグが見つかった場合\n",
    "    if div_data_fixed:\n",
    "        # <p class=\"m-txt-copy hide-sp\">タグを除外\n",
    "        excluded_tags = div_data_fixed.find_all('p', {'class': 'm-txt-copy hide-sp'})\n",
    "        \n",
    "        # 除外したいタグをすべて削除\n",
    "        for tag in excluded_tags:\n",
    "            tag.decompose()\n",
    "        \n",
    "        # 取得した内容をXMLとして保存\n",
    "        xml_content += div_data_fixed.prettify() + \"\\n\"\n",
    "\n",
    "        # XMLをファイルに保存 (バイトデータとして保存)\n",
    "        with open(\"output.xml\", \"wb\") as file:\n",
    "            file.write(xml_content.encode('utf-8'))\n",
    "        print(\"特定のdivデータをXMLファイルに格納しました。\")\n",
    "    else:\n",
    "        print(\"指定された<div data-fixed=''>が見つかりませんでした。\")\n",
    "else:\n",
    "    print(f\"URLにアクセスできません。ステータスコード: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd5926c6-fa3e-41ee-b5b9-ed9a849a2ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "除外された特定のdivデータをXMLファイルに格納しました。\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 取得したいURLを指定\n",
    "url = \"https://www.kirinholdings.com/jp/domains/health_science/kyowahakko-bio/\"\n",
    "\n",
    "# URLからHTMLを取得\n",
    "response = requests.get(url)\n",
    "\n",
    "# レスポンスが成功したか確認\n",
    "if response.status_code == 200:\n",
    "    # バイトデータ (response.content) を使い、from_encoding を指定\n",
    "    soup = BeautifulSoup(response.content, 'lxml', from_encoding='utf-8')\n",
    "\n",
    "    # <div data-fixed=\"\">タグを取得\n",
    "    div_data_fixed = soup.find('div', {'data-fixed': True})\n",
    "    \n",
    "    # 特定のタグが見つかった場合\n",
    "    if div_data_fixed:\n",
    "        # <p class=\"m-txt-copy hide-sp\">タグを除外\n",
    "        excluded_tags = div_data_fixed.find_all('p', {'class': 'm-txt-copy hide-sp'})\n",
    "        \n",
    "        # 除外したいタグをすべて削除\n",
    "        for tag in excluded_tags:\n",
    "            tag.decompose()\n",
    "        \n",
    "        # 取得した内容をXMLとして保存\n",
    "        xml_content = div_data_fixed.prettify() + \"\\n\"\n",
    "        \n",
    "        # XMLをファイルに保存 (バイトデータとして保存)\n",
    "        with open(\"output_excluded.xml\", \"wb\") as file:\n",
    "            file.write(xml_content.encode('utf-8'))\n",
    "        print(\"除外された特定のdivデータをXMLファイルに格納しました。\")\n",
    "    else:\n",
    "        print(\"指定された<div data-fixed=''>が見つかりませんでした。\")\n",
    "else:\n",
    "    print(f\"URLにアクセスできません。ステータスコード: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27803ee0-25de-49c8-842f-2cd5551f8ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
